{"pages":[{"title":"关于","text":"关于我一名普通的学生。","link":"/about/index.html"}],"posts":[{"title":"CSAPP第七章Linking","text":"Object FilesObject file 有三种形式 Relocatable object file Executable object file Shared object file Relocatable Object FilesCSAPP 书中选取的是ELF格式。ELF从16字节的序列作为起始，这16字节规定了字的大小和机器的字节顺序(大端或者小端)，ELH header剩下的内容帮助连接器分析解释这个object file。而各种sections的信息，如位置和大小，由section header table 给出，它包含了每个sections固定大小的入口。 sections的类型 .te xt编译后的机器码 .rodata只读数据，例如printf中的format strings 或者switch的jump tables .data初始化的全局和静态变量 .bss未初始化的全局和静态变量，并且这个section并不占据object file的实际磁盘空间，当程序运行时，这些变量会申请内存并初始化为0 .symtabsymbol table，包含了函数和全局变量的信息。 .rel.rext是.text section中需要重定位的位置的列表，例如外部函数或者外部的局部变量。和并不包含没有用到的指令。 .rel.data包含这个object module中定义或者引用的全局变量的重定位信息。 .debugdebugging symbol，只有用了-g选项编译才会产生 .line .strtab字符串表","link":"/2021/09/16/CSAPP%E7%AC%AC%E4%B8%83%E7%AB%A0Linking/"},{"title":"《STL源码剖析》第二章问题杂烩","text":"近期开始阅读侯捷《STL源码剖析》，第二章有许许多多我遇到的问题，在此汇总一下 Placement new书中的使用 语法new的语法中有placement_params可选参数，当传递给new这个参数时，就是placement_new 作用placement_new就是直接将一块内存空间指定给new，new直接在这些内存上执行构造 样例1234567891011121314//样例1void* operator new(std::size_t, void*)//样例2// within any block scope...{ alignas(T) unsigned char buf[sizeof(T)]; // Statically allocate the storage with automatic storage duration // which is large enough for any object of type `T`. T* tptr = new(buf) T; // Construct a `T` object, placing it directly into your // pre-allocated storage at memory address `buf`. tptr-&gt;~T(); // You must **manually** call the object's destructor // if its side effects is depended by the program.} // Leaving this block scope automatically deallocates `buf`. false_type和true_type书中的使用书中是用来方便__destory_aux的调用，个人理解为重载，可以由__destroy直接调用两个不同作用的函数，通过false_type和true_type这两个不同的变量类型。 未完待续….","link":"/2021/08/18/STL%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90%E7%AC%AC%E4%BA%8C%E7%AB%A0%E9%97%AE%E9%A2%98%E6%9D%82%E7%83%A9/"},{"title":"Streaming Graph Neural Networks笔记","text":"Streaming Graph Neural Networks笔记 该论文提出了DGNN模型 The update componentinteract unit$$ e(t)=act(W_1 \\cdot u_{v_s}(t-)+W_2 \\cdot u_{v_g}(t-)+b_e)$$ 其中$u_{v_s}$和$u_{v_g}$是点在时间t之前的特征，$W_1$和$W2$以及$b_e$是神经网络的参数,$act(\\cdot)$是激活函数。$e(t)$包含了${v_s, v_g, t}$的交互的信息。 update unitupdata unit将经过intercact unit的节点信息更新。当许多interactions作用于同一节点时，update unit将遗忘以前的interactions。这操作和LSTM很像，所以论文魔改了LSTM其中target node和source node是用的两个不同的update unit，结构相同，参数不同。update unit和LSTM不同的地方在蓝色虚线中，相当于改变了LSTM中$C$的输入。公式：$$C^I_v(t-1) = tanh(W_d \\cdot C_v(t-1)+b_d)$$ $$\\hat{C}^I_v(t-1)=C^I_v(t-1)*g(\\Delta t)$$ $$C^T_v(t-1)=C_v(t-1)-C^I_v(t-1)$$ $$C^*_v(t-1)=C^T_v(t-1)+\\hat{C}^I_v(t-1)$$其中$C^I_v$由神经网络生成，代表短期记忆，短期记忆会随着时间遗忘，所以短期记忆会乘上$g(\\Delta t)$ 得出$\\hat{C}^I_v$，来表示遗忘后的短期记忆，$g(\\Delta t)$是遗忘函数，$\\Delta t$越长，其值越小。$C^T_v$是长期记忆，值不随时间而改变。$C^*_v(t-1)$是调整之后的$C$，作为标准LSTM的输入。后面就是LSTM的内容了。$$ f_t=\\delta(W_f \\cdot e(t) + U_f \\cdot h_v(t-1)+b_f) $$$$i_t=\\delta (W_i \\cdot e(t) + U_i \\cdot h_v(t-1)+b_i)$$$$o_t = \\delta (W_o \\cdot e(t) + U_o \\cdot h_v(t-1)+b_o)$$$$\\tilde{C}(t)=tanh(W_c \\cdot e(t) +U_c \\cdot h_v(t-1)+b_c)$$$$C_v(t)=f_t * C^*_v(t-1)+i_t *\\tilde{C}_v(t)$$$$ h_v(t) = o_t * tanh(C_v(t))$$ 值得注意的是，target和source node是用了两个不同的update unit。source update 只更新source node，同理target update The merge unit该unit是将当前生成的信息与之前的信息融合，形成$u_v(t)$source node 公式：$$u_{v_s}(t) = W^S \\cdot h^S_{v_s}+W^g \\cdot h^g_{v_s}(t-)+b_u$$同理target node:$$u_{v_g}(t)=W^S*h^S_{v_g}(t-)+W^g \\cdot h^g_{v_g}(t) +b_u$$ 整个update component 的流程： The propagation componentupdate component 只考虑连接建立起来时，两点的相互作用，而propagation component 则是将这两点的交互信息带入邻居节点中。影响邻居节点的方式不是像update component 那样修改cell memory的历史，而是给cell memory增添新的信息。 符号表示$N^S(\\cdot)$表示该点的source邻居的集合，$N^g(v_g)$表示该点target邻居的集合。$$N(v_s)=N^s(v_s) \\cup N^g(v_s)$$$$N(v_g)=N^s(v_g) \\cup N^g(v_g)$$ prop unit四种类型 source node $v_s$到它的source邻居$N^s(v_s)$ source ndoe $v_s$到它的target邻居$N^g(v_s)$ target node $v_g$到它的source邻居$N^s(v_g)$ target node $v_g$到它的target邻居$N^g(v_g)$ 这四种类型的prop unit结构相同，参数不同。在论文中只提供了第一种的描述，其他类型类比即可。 propagate interaction information forrmulations$$C^s_{v_x}(t)=C^s_{v_x}(t-)+f_a(u_{v_x}(t-), u_{v_s}(t-))\\cdot g(\\Delta ^s_t) \\cdot h(\\Delta ^s_t) \\cdot \\hat{W}^s_s \\cdot e(t)$$$$h^s_{v_x}(t)=tanh(C^s_{v_x}(t))$$其中$v_x\\in N^s(v_s)$，而$\\Delta^s_t=t-tx$代表了当前时间$t$与节点$v_x$与节点$v_s$发生交互的时间$t_x$的时间间隔。$f_a$是和update component中定义$f_a$是同一个衰减函数。$W^s_s$是传给source邻居交互信息的线性变换。 函数h的定义$$h(\\Delta ^s_t)=\\begin{cases}1, \\Delta^s_t\\le \\tau,\\0, otherwise.\\end{cases}$$$\\tau$是一个超参，如果时间间隔大于这个值，那么就代表时间间隔太大了。我们就不会向这个点传递交互信息。 $f_a$的定义$f_a$是捕获$v_s$和$v_x$之间连接强度的注意力函数$$f_a(u_{v_x}(t-),u_{v_s}(t-))=\\dfrac{exp(u_{v_x}(t-)^Tu_{v_s}(t-))}{\\sum_{v\\in N^s(v_s)} exp(u_v(t-)^Tu_{v_s}(t-))}$$ propagation流程图 参数学习Parameter learning for link predictionlink prediction 就是预测下一个连接关系。 projection matrixprojection matrix $P$是关于因变量(?英文名为response values)和预测值的映射。$$\\hat{y} = Py$$ $u^s_{v_s}(t-)$和$u^g_{v_g}(t-)$论文中将$u_{v_s}(t-)$和$u_{v_g}$通过$P$投射到$u^s_{v_s}(t-)$和$u^g_{v_g}(t-)$ $$u^s_{v_s}(t-)=P^s \\cdot u_{v_s}(t-)$$$$u^g_{v_g}(t-)=P^g \\cdot u_{v_g}(t-)$$ loss function对于$v_s$和$v_g$出现的概率，用$$\\sigma (u^s_{v_s}(t-)^T u^g_{v_g}(t-)) $$$\\sigma(\\cdot)$代表sigmod函数。loss function: $$ J((v_s, v_g, t))=-log(\\sigma (u^s_{v_s}(t-)^T u^g_{v_g}(t-)))-Q \\cdot \\mathbb E_{v_n\\sim P_n(v)}log(\\sigma (u^s_{v_s}(t-)^T u^g_{v_g}(t-)))$$ $Q$是负采样的数量，$P_n{v}$是一个负采样的分布。 total loss: $$ \\sum_{e\\in \\mathcal{E}(T)}J(e) $$ 其中$\\mathcal{E}(T)$代表时间$T$之前的所有交互。 训练方法采用mini-batch进行优化，并且mini-batch中边的样本是通过交互的时间序列采样的。mini-batch的loss通过所有在其中的交互计算的。负采样的分布$P_n(v)$是一个在mini-batch外所有点的标准分布，包括了交互的点和受影响的点。","link":"/2021/09/18/Streaming-Graph-Neural-Networks%E7%AC%94%E8%AE%B0/"},{"title":"《STL源码剖析》中void (* set_malloc_handler(void (*f)()))()分析","text":"问题在阅读《STL源码剖析》的时候，遇到了这么一个函数 先不说函数的意思，函数的形式我就没看懂…好多好多括号… 函数指针函数指针就是指向函数的指针… 1234567891011121314151617181920212223242526272829/* 例一：函式指標直接呼叫 复制自WIKI*/# ifndef __cplusplus # include &lt;stdio.h&gt;# else # include &lt;cstdio&gt;# endifint max(int x, int y){ return x &gt; y ? x : y;}int main(void){ /* p 是函数指针*/ int (* p)(int, int) = &amp; max; // &amp;可以省略 int a, b, c, d; printf(&quot;please input 3 numbers:&quot;); scanf(&quot;%d %d %d&quot;, &amp; a, &amp; b, &amp; c); /* 與直接呼叫函式等價，d = max(max(a, b), c) */ d = p(p(a, b), c); printf(&quot;the maxumum number is: %d\\n&quot;, d); return 0;} 在《STL源码剖析》里的这个函数，实际上使用了函数指针作为参数 1void (*f)() 函数指针作为返回值1int (*test(int))(int, int) 阅读方法是由内向外读，首先test有形参列表，所以是一个函数，并且参数只需要一个int，然后test的前面有一个*，所以返回一个指针，(*test(int))作为一个指针，有形参列表(int, int)，所以这个指针指向函数，并且这个函数返回int类型的值。 分析1void (* set_malloc_handler(void (*f)()))() 首先，set_malloc_handler作为函数名，有一个形参，是void(*f)()，void(f)()是一个函数指针类型，指向返回值为void，参数为空的函数。这说明set_malloc_handler是函数，并且前面有，所以要返回指针，指针后面接着形参列表，为空()，说明是指向函数的指针，并且指向的函数返回类型为void。","link":"/2021/08/20/%E5%87%BD%E6%95%B0%E6%8C%87%E9%92%88/"},{"title":"动态图神经网络综述笔记(一)分类","text":"本文是动态网络综述：Foundations and modelling of dynamicnetworks using Dynamic Graph NeuralNetworks: A survey 的笔记 分类根据时间粒度分类 Static没有时间信息 Edge-weighted时间信息被当做标签存放在静态网络的边/点中，最直观的例子是静态网络中边最后一次活跃时间的标签。 Discretue以离散的时间间隔表示时间信息，可以用不同时间间隔的快照来表示。 Continuous没有时间间隔，这种表示方式承载着最多的信息，但同时也最复杂。 后两者主要用来建立动态网络。 表示方法 Discrete RepresentationT代表快照的序号 Continuous Representation The event-basedrepresentationui和vi是一对连接的点，ti是时间戳，代表连接开始的时间，△i是事件(连接)持续的时间 The contact sequence representation是上一种的简化，在这种连接中，连接是instantaneous(瞬时)的，所以没有连接的持续时间。 The graph stream representation其中 ui,vi是一对连接的点，ti是事件发生的时间，最后的符号如果为1，代表边的的加入，-1代表边的删除。 根据连接时间分类 Interaction Temporal Evolving Strictly evolving 从上到下连接时间为0-无穷 根据点的动态性区分 Static点的数量始终不变 Dynamic点可能消失或出现 Growin是一种特殊的Dynamic，点只能增长 动态网络CUBEtemporal可以在没有连接的情况下存在，但对于evolving等许多网络来说不可以，当连接不存在时，点也就不存在了。三种分类可以形成一个3D的分类图 动态网络模型","link":"/2021/09/13/%E5%8A%A8%E6%80%81%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%BC%E8%BF%B0%E7%AC%94%E8%AE%B0(%E4%B8%80)/"},{"title":"动态图神经网络综述笔记(二)动态图神经网络","text":"笔记第二部分 Dynamic graph neural networks动态图网络有深度的时间序列编码，能将邻居节点聚合起来。 定义如果一个深度表征学习将邻居节点信息聚集起来作为结构，那么这就是一个动态图神经网络在离散的情况下，DGNN是GNN和时间序列的组合。如果是连续的，情况就会有些多变，聚集节点不能再用传统的GNN的方法。 DGNN的种类 Pseudo-dynamic这个方法改变的是网络的拓扑结构，而不是时间。 Discrete编码网络使用了快照，并且每次快照编码一次。 Continuous遍历网络是以边和边的形式，所以它完全独立于任何的快照尺寸。Pserudo-dynamic models pseudo-dynamic contains dynamic processes, but thedynamic properties of the model are not fit to the dynamicdata. 按照我的理解，这种方式只适用于数据不会改变的情况… Edge-weighted models是将动态网络转化为edge-weighted network 然后使用静态的GNN在上面。例如TDGNN Discrete dynamic graph neural networks给出一组离散的图通过GNN形成z代表在时间t中的i节点通过函数f，生成当前的h，f可以是RNN和自注意力机制也可以这样表示：","link":"/2021/09/14/%E5%8A%A8%E6%80%81%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%BC%E8%BF%B0%E7%AC%94%E8%AE%B0-%E4%BA%8C-%E5%8A%A8%E6%80%81%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"title":"通过右移2的幂次实现被除数为负数的除法","text":"在阅读CSAPP的时候，看了练习题3.20的答案，说到被除数为负数的时候需要加偏置。 WHY?总结了一下CSAPP中的解释（书P73）计算机实现除法的时候，需要的时钟周期很长，所以当除数为2的幂次的时候，就可以使用算术右移来实现除法。 被除数非负的情况整数的除法总是舍入到零的，比如2.3舍入到2， -1.5舍入到-1。当被除数为正数的时候，算术右移来执行除法，结果自然是向零舍入的。（因为直接把后面右移的数字舍弃了） 被除数负数的情况但是当被除数是负数的时候，会出现向下舍入，比如-1.5会被舍入为-2。为了解决这个问题，我们需要先对被除数加上一个偏置数。当除数为2的k次幂的时候，偏置量为2^k - 1这样就可以解决被除数为负数的舍入问题。以下为CSAPP中的推导 个人简单理解假设除数为2的k次幂，代表我们要右移k位，当这k位都为0的时候，代表不需要舍入，能够整除，但是当k位有任何一位为1的时候，代表不能整除，需要做舍入处理。如果不加偏置，就代表我们要直接舍弃最低的k位，对不需要右移的最低位毫无影响，那么对于负数来说。如果加上偏置，那么需要舍入的时候，对于第k+1位，会有一个来自低位的进位，这个时候数值自然就会向0舍入。","link":"/2021/07/19/%E9%80%9A%E8%BF%87%E5%8F%B3%E7%A7%BB2%E7%9A%84%E5%B9%82%E6%AC%A1%E5%AE%9E%E7%8E%B0%E9%99%A4%E6%B3%95/"},{"title":"计算机网络笔记3.7","text":"Classic TCP Congestion Control这种方法是当网络拥塞时，TCP减少发送的速率。当网络不拥塞时，TCP增加发送速率。这样就带来三个问题 当遇到网络阻塞时，TCP如何限制发送的速率？ TCP如何感知拥塞？ 当发送方感知到了端对端的拥塞时，应当采用什么算法改变速率？ 前提概念$LastByteRead$ 应用进程从缓存中独处的数据流的最后一个字节的编号$LastByteRcvd$ 从网络中到达的并且已放入主机接受缓存中的数据流最后一个字节的编号$rwnd$为接受窗口，是表明当前缓存中还有多少剩余的空间$$ rwnd = RcvBuffer -[LastByteRcvd - LastByteRead] $$ $ cwnd $ 为拥塞窗口，它对发送方向网络中发送流量的速率进行了限制 TCP发送方限制发送连接流量$ cwnd $ 会对一个TCP发送方能向防落中发送流量的速率进行限制。在一个发送方中未被确认的数据流不会超过$cwnd$和$rwnd$中的最小值。$$ LastByteSent - LastByteAcked \\le min{cwd, rwnd}$$如果rwnd足够大的时候，发送方能发送的未被确认的数据量取决于$cwdn$，所以通过调节$cwdn$的值，就可以调节发送方向它连接发送数据的速率。","link":"/2021/09/15/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%AC%94%E8%AE%B03-7/"},{"title":"记录BUG_孤儿进程","text":"记录bug 今天写多进程的时候遇到了这样一个bug，我通过fork创建子进程，然后再让子进程fork创建子进程…直到一个终止条件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990// #include &quot;kernel/types.h&quot;// #include &quot;kernel/stat.h&quot;// #include &quot;user/user.h&quot;#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;stdlib.h&gt;#include &lt;sys/wait.h&gt;#define READMODE 0#define WRITEMODE 1int main(){ // int primers_array[40]; // for(int i = 2; i &lt;= 35; i++) // primers_array[i] = i; int p[2]; pipe(p); int read_pipe = p[READMODE]; int write_pipe = p[WRITEMODE]; close(0); int pppid = 0; if( (pppid = fork()) == 0) { while(1) { printf(&quot;pid: %d\\n&quot;, getpid()); pipe(p); close(write_pipe); int base_num; if(read(read_pipe, &amp;base_num, sizeof(int)) == 0) { printf(&quot;pid %d 我死了\\n&quot;,getpid()); close(p[READMODE]); close(p[WRITEMODE]); close(read_pipe); exit(0); } printf(&quot;prime %d\\n&quot;, base_num); if(base_num == 31) { close(p[READMODE]); close(p[WRITEMODE]); close(read_pipe); exit(0); } int pid = fork(); if(pid &gt; 0) { close(p[READMODE]); int x; while(read(read_pipe, &amp;x, sizeof(int))) { if(x % base_num != 0) { write(p[WRITEMODE], &amp;x, sizeof(int)); } } close(read_pipe); int status = 0; printf(&quot;pid %d done\\n&quot;, getpid()); exit(0); } else { read_pipe = p[READMODE]; write_pipe = p[WRITEMODE]; } } } else { close(p[READMODE]); for(int i = 2; i &lt;= 35; i++) { write(p[WRITEMODE], &amp;i, sizeof(int)); } close(p[WRITEMODE]); int wpid; int status = 0; printf(&quot;结束了&quot;); exit(0); } exit(0);} 发现最后进程都退出之后，命令行不动，除非输入一些东西或者放大缩小终端界面。。。我目测原因是因为这样链式fork，并且每个父进程都在子进程前面exit，就会导致子进程成为孤儿进程，孤儿进程由init操作系统管理，也就是说，孤儿进程的父进程会变成system，不再是bash，原因应该是最后一个进程exit后，bash没有关于这个进程的信息，所以不会变化…但是具体原因不是很清楚，需要学完操作系统后再看。而且当我使用wait之后，所有父进程都卡住了… An orphan process is a computer process whose parent process has finished or terminated, though it remains running itself. In a Unix-like operating system any orphaned process will be immediately adopted by the special init system process: the kernel sets the parent to init. This operation is called re-parenting and occurs automatically. Even though technically the process has the “init” process as its parent, it is still called an orphan process since the process that originally created it no longer exists. In other systems orphaned processes are immediately terminated by the kernel. In modern Linux systems, an orphan process may be reparented to a “subreaper” process instead of init. update:原因是我没有关闭管道，导致阻塞…还要记得父进程要wait子进程。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;stdlib.h&gt;#include &lt;sys/wait.h&gt;#define READMODE 0#define WRITEMODE 1int main(){ int p[2]; pipe(p); int read_pipe = p[READMODE]; int write_pipe = p[WRITEMODE]; close(0); int pppid = 0; if( (pppid = fork()) == 0) { while(1) { pipe(p); close(write_pipe); int base_num; if(read(read_pipe, &amp;base_num, sizeof(int)) &lt;= 0) { close(p[READMODE]); close(p[WRITEMODE]); close(read_pipe); exit(0); } printf(&quot;prime %d\\n&quot;, base_num); int pid = fork(); if(pid &gt; 0) { close(p[READMODE]); int x; while(read(read_pipe, &amp;x, sizeof(int))) { if(x % base_num != 0) { write(p[WRITEMODE], &amp;x, sizeof(int)); } } close(p[WRITEMODE]); close(read_pipe); wait((int*)0); exit(0); } else { read_pipe = p[READMODE]; write_pipe = p[WRITEMODE]; } } } else { close(p[READMODE]); for(int i = 2; i &lt;= 35; i++) { write(p[WRITEMODE], &amp;i, sizeof(int)); } close(p[WRITEMODE]); wait((int*)0); exit(0); } exit(0);}","link":"/2021/09/19/%E8%AE%B0%E5%BD%95BUG-%E5%AD%A4%E5%84%BF%E8%BF%9B%E7%A8%8B/"},{"title":"栈在虚拟内存和物理内存映射的增长问题","text":"分段为了解决仅仅使用基址和界限寄存器将进程重定位到物理内存区域，会产生大量的空隙，导致内存没有被有效利用。所以产生了分段的概念，在MMU中，不仅仅一对引入基址和界限寄存器，而是给地址空间内的每个逻辑段一对。一段是地址空间里的一个连续定长的区域。并且只有已用的内存才会在物理内存中分配空间。 如何引用段虚拟地址被分为两个部分，第一个部分表示哪个段，第二个部分表示在段内的地址偏移量。如果地址空间内有三个段，那么需要前两位来表示段，剩下的表示偏移。如上图所示，它指明了第一个段中104（十进制）偏移的位置。也有隐式的方法来表示，比如硬件可以通过地址的产生方式来确定段。 虚拟地址的地址转换正向增长的段，其虚拟地址对应的物理地址很好计算。只需要将偏移量加在Base地址上就行。对于栈这种增长方向为负的来说，需要先计算段的最大偏移量，比如现在有要访问的虚拟地址是15KB（以最开始的两个图为例），那么它应该映射到物理内存的27KB位置，这个虚拟地址的二进制形式是 11 1100 0000 0000前两位是段的表示，那么后12位则是偏移量的可用区域，可用区域为 11 0000 0000 0000 到 11 1111 1111 1111（当然哪些真正可用需要看规定的段的大小）那么最大偏移量为4KB($2^{12}$)，由于栈是负增长的，而虚拟地址中的偏移量是表示距离 11 0000 0000 0000的大小，而距离栈的偏移量的Base应该是虚拟地址的偏移量减去4KB，也就是3KB-4KB=-1KB，从而得到27KB。","link":"/2021/09/24/%E6%A0%88%E5%9C%A8%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%E5%92%8C%E7%89%A9%E7%90%86%E5%86%85%E5%AD%98%E6%98%A0%E5%B0%84%E7%9A%84%E5%A2%9E%E9%95%BF%E9%97%AE%E9%A2%98/"},{"title":"mit 6.S081 Lab2 System Call","text":"做这个实验之前需要把 xvb book 第二章和第四章的 4.3 4.4看了， 有助于理解。 System call tracing实验大意实现trace指令， trace接受mask以及调用的程序和它的参数。mask代表要追踪哪些系统调用，这些都可以在kernel/syscall.h中看到 过程大致思路是通过trace函数将进程中新添加的trace_mask设置好，当syscall调用时，通过trace_mask和被调用的系统调用的mask进行相与，如果大于零，就打印该系统调用的信息。 这个lab已经提供了user/trace.c这个函数，所以不需要自己写，但是需要在user/user.h，user/usys.pl，以及kernel/syscall.h添加声明，还需要在kernel/syscall.c中的函数指针数组中，添加后面需要我们自己在 kernel/sysproc.c添加的sys_trace。 12// user/user.hint trace(int); 12// user/usys.plentry(&quot;trace&quot;); 12// kernel/syscall.h#define SYS_trace 22 12345678910111213141516171819202122232425//kernel/syscall.cstatic uint64 (*syscalls[])(void) = {[SYS_fork] sys_fork,[SYS_exit] sys_exit,[SYS_wait] sys_wait,[SYS_pipe] sys_pipe,[SYS_read] sys_read,[SYS_kill] sys_kill,[SYS_exec] sys_exec,[SYS_fstat] sys_fstat,[SYS_chdir] sys_chdir,[SYS_dup] sys_dup,[SYS_getpid] sys_getpid,[SYS_sbrk] sys_sbrk,[SYS_sleep] sys_sleep,[SYS_uptime] sys_uptime,[SYS_open] sys_open,[SYS_write] sys_write,[SYS_mknod] sys_mknod,[SYS_unlink] sys_unlink,[SYS_link] sys_link,[SYS_mkdir] sys_mkdir,[SYS_close] sys_close,[SYS_trace] sys_trace,//新添加的函数指针，指向sysproc.c中的sys_trace} 然后向kernel/proc.h 中的 struct proc 添加 int trace_mask 1234struct proc { ...... int trace_mask;}; user调用trace时，实际上调用的是sys_trace，所以我们需要添加在 kernei/sysproc.c的sys_trace函数。。 123456789101112131415161718192021222324// kernel/sysproc.c//这里我将主要的函数写在了 kernel/proc.c中uint64sys_trace(void){ int trace_mask = 0; if(argint(0, &amp;trace_mask) &lt; 0)// 通过argint读取调用trace时所传入的参数 return -1; return trace(trace_mask);}//kernel/proc.cint trace(int mask){ struct proc *p = myproc();//获取当前进程 p-&gt;trace_mask = mask;// 将当前进程的trace_mask设置为 mask return 0;}//kernel/defs.h//在proc定义的函数，需要在 defs.h中添加声明，这样sysporc.c中才可以使用//proc.h.......int trace(int); 我们还需要在调用fork生成子进程的时候，将父进程的trace_mask复制给子进程 1np-&gt;trace_mask = p-&gt;trace_mask; 最后需要在syscall中，判断是否被调用的函数包含在mask中，如果包含在mask中，则输出信息，因为输出信息需要输出系统调用的函数名，所以还需要添加一个字符串数组。 123456789101112131415161718192021222324252627282930313233343536373839404142434445kernel/syscall.hchar * fun_name[35]={ &quot;null&quot;,//在函数的mask都是从1开始的，所以null作为占位符。 &quot;fork&quot;, &quot;exit&quot;, &quot;wait&quot;, &quot;pipe&quot;, &quot;read&quot;, &quot;kill&quot;, &quot;exec&quot;, &quot;fstat&quot;, &quot;chdir&quot;, &quot;dup&quot;, &quot;getpid&quot;, &quot;sbrk&quot;, &quot;sleep&quot;, &quot;uptime&quot;, &quot;open&quot;, &quot;write&quot;, &quot;mknod&quot;, &quot;unlink&quot;, &quot;link&quot;, &quot;mkdir&quot;, &quot;close&quot;, &quot;trace&quot;, &quot;sysinfo&quot;};voidsyscall(void){ int num; struct proc *p = myproc(); num = p-&gt;trapframe-&gt;a7; int mask_value = 1 &lt;&lt; num; if(num &gt; 0 &amp;&amp; num &lt; NELEM(syscalls) &amp;&amp; syscalls[num]) { p-&gt;trapframe-&gt;a0 = syscalls[num](); if((mask_value &amp; (p-&gt;trace_mask)) != 0)//如果当前函数的mask在trace_mask中,就打印信息，a0寄存器中包含函数的返回值。 printf(&quot;%d: syscall %s -&gt; %d\\n&quot;, p-&gt;pid, fun_name[num], p-&gt;trapframe-&gt;a0); } else { printf(&quot;%d %s: unknown sys call %d\\n&quot;, p-&gt;pid, p-&gt;name, num); p-&gt;trapframe-&gt;a0 = -1; } 这样我们就完成了lab2的第一个实验。 Sysinfo实验大意实现sysinfo，获取系统中有多少非UNUSED的进程和空闲内存字节数。 过程和上一个system call 实验一样，需要将sysinfo声明在各个文件中，实验介绍中已经给出来了。然后就是获取当前不是UNUSED的进程，比较简单。 1234567891011121314// kernel/proc.cint count_proc(void){ int count = 0; struct proc *p; for(p = proc; p &lt; &amp;proc[NPROC]; p++) //proc是全局变量，是进程的数组，NPROC是进程的个数 { acquire(&amp;p-&gt;lock);// 先给所查询的进程上锁 if(p-&gt;state != UNUSED)// 如果state不是 unused，则count++ count ++; release(&amp;p-&gt;lock); } return count;} 然后就是获取空闲内存字节数， xv6是通过freelist管理的内存，每个页大小为PAGESIZE，相关代码可以参考 kernel/kalloc.c中的kalloc 123456789101112131415161718// Allocate one 4096-byte page of physical memory.// Returns a pointer that the kernel can use.// Returns 0 if the memory cannot be allocated.void *kalloc(void){ struct run *r; acquire(&amp;kmem.lock); r = kmem.freelist; // 获取freelist的头指针 if(r) kmem.freelist = r-&gt;next; //获取一个freelist中的页 release(&amp;kmem.lock); if(r) memset((char*)r, 5, PGSIZE); // fill with junk return (void*)r;} 通过阅读这个代码，就可以知道想计算空闲内存的大小，只需要计算freelist中有多少个页就行了。 1234567891011121314uint64 get_freemem_num(void){ struct run *r; uint64 num = 0; acquire(&amp;kmem.lock);// 上锁 r = kmem.freelist; while(r) { num ++; // 如果当前指针指向的是一个页，num++ r = r-&gt;next; } release(&amp;kmem.lock); return num * PGSIZE; // 通过页的个数 乘上 页的大小来获取内存空闲大小} 至此，我们需要的两个函数就写完了，剩下的就是完成kernel/sysproc.c 中的sysinfo函数了，需要注意的是，当程序调用系统函数的时候，进入的是 kernel mode 无法通过 user mode 中的虚拟地址直接将数据存入虚拟地址所指向的物理地址，因为目前xv6是有一个全局的kernel page table，其中没有每个进程的user table 的映射，所以需要使用copyout来获取指定page table中的物理地址，并将数据存入进去。 1234567891011121314151617181920uint64sys_sysinfo(void){ uint64 st; struct proc *p = myproc(); struct sysinfo info; if(argaddr(0, &amp;st) &lt; 0) { printf(&quot;sys_sysinfo: get addr error\\n&quot;); return -1; } info.freemem = get_freemem_num(); info.nproc = count_proc(); if(copyout(p-&gt;pagetable, st, (char *)&amp;info, sizeof info) &lt; 0) // 获取物理地址，并将info存入其中。 { printf(&quot;sys_sysinfo: copyout error\\n&quot;); return -1; } return 0;} 在添加函数的时候，不要忘记取 defs.h中添加对应的声明。 我们lab2整个就完成了^_^","link":"/2021/10/01/mit-6-S081-Lab2-System-Call/"},{"title":"mit 6.S081 Lab3 page tables","text":"这一次实验需要读xv6 book中的第三章，而且需要好好读… Print a page table实验大意实现vmprint，将指定的page talbe中的信息打印出来 过程xv6中三级页表，每级page table大小是 page 大小，并且可以容纳512个pte，所以每级Page table 需要在va中占据9位来表明PTE的位置。并且可以使用这个PTE跳转到所需要的Page table的位置。这个实验需要读一下 kernel/vm.c 的walk函数。 12345678910111213141516171819202122232425262728293031// Return the address of the PTE in page table pagetable// that corresponds to virtual address va. If alloc!=0,// create any required page-table pages.//// The risc-v Sv39 scheme has three levels of page-table// pages. A page-table page contains 512 64-bit PTEs.// A 64-bit virtual address is split into five fields:// 39..63 -- must be zero.// 30..38 -- 9 bits of level-2 index.// 21..29 -- 9 bits of level-1 index.// 12..20 -- 9 bits of level-0 index.// 0..11 -- 12 bits of byte offset within the page.pte_t *walk(pagetable_t pagetable, uint64 va, int alloc){ if(va &gt;= MAXVA) panic(&quot;walk&quot;); for(int level = 2; level &gt; 0; level--) { pte_t *pte = &amp;pagetable[PX(level, va)]; if(*pte &amp; PTE_V) { // 如果当且页有效的话 pagetable = (pagetable_t)PTE2PA(*pte); //获取pte中的pa } else { if(!alloc || (pagetable = (pde_t*)kalloc()) == 0) return 0; memset(pagetable, 0, PGSIZE); *pte = PA2PTE(pagetable) | PTE_V; } } return &amp;pagetable[PX(0, va)];} 那么vmprint大体思路就出来了，因为pagetable中有512个pte，我们只需要遍历pte，并且如果pte指向的page 还是page table 那么就递归的调用vmprint就行，这里为了实现递归调用，我们需要一个tool函数。 1234567891011121314151617181920212223242526272829//print ptes and pas from a pagetablevoidvmprint(pagetable_t pagetable){ printf(&quot;page table %p\\n&quot;, pagetable); __vmprint_tool(pagetable, 1);}// a tool function for vmprintvoid__vmprint_tool(pagetable_t pagetable, int depth){ for(int i = 0; i &lt; 512; i++){ pte_t pte = pagetable[i]; if((pte &amp; PTE_V) == 0) continue; for(int j = 0; j &lt; depth; j++)// 根据当前的深度打印.. { if(j != 0) printf(&quot; &quot;); printf(&quot;..&quot;); } printf(&quot;%d: pte %p pa %p\\n&quot;, i, pte, PTE2PA(pte)); if((pte &amp; PTE_V) &amp;&amp; (pte &amp; (PTE_R|PTE_W|PTE_X)) == 0) {//如果一个pte的标志位中，PTE_R,PTE_W,PTE_X都为零，就代表指向的是一个page table __vmprint_tool((pagetable_t) PTE2PA(pte), depth + 1); } }} 最后需要在exec函数中添加一段代码，让系统初始化完成后，执行第一个进程时调用vmprint。 12345// kernel/exec.c if(p-&gt;pid == 1) { vmprint(p-&gt;pagetable); } A kernel page table per process实验大意xv6中有一个全局Kernel page table 现在该实验让你实现每个进程都有自己的kernel page table 过程这个实验比较麻烦，但实际上跟着hints来就可以。 首先要将 kernel/proc.h中的 struct proc 增加一个新的变量来存储kernel proc 12345678910111213141516171819202122232425// 这里我把它命名为kernel_pagetablestruct proc { struct spinlock lock; // p-&gt;lock must be held when using these: enum procstate state; // Process state struct proc *parent; // Parent process void *chan; // If non-zero, sleeping on chan int killed; // If non-zero, have been killed int xstate; // Exit status to be returned to parent's wait int pid; // Process ID // these are private to the process, so p-&gt;lock need not be held. uint64 kstack; // Virtual address of kernel stack uint64 sz; // Size of process memory (bytes) uint64 kstack_pa; //kstack pa in kernel pagetable pagetable_t pagetable; // User page table pagetable_t kernel_pagetable; // kernel_pagetable struct trapframe *trapframe; // data page for trampoline.S struct context context; // swtch() here to run process struct file *ofile[NOFILE]; // Open files struct inode *cwd; // Current directory char name[16]; // Process name (debugging)}; 然后我们需要魔改allocproc，这个函数是用来给一个在proc数组里，并且为UNUSED的进程初始化的， 这里我们需要添加关于kernel page table的代码， 全局page talbe初始化是用的kvminit函数。 Early in the boot sequence, main calls kvminit (kernel/vm.c:22) to create the kernel’s pagetable. This call occurs before xv6 has enabled paging on the RISC-V, so addresses refer directly tophysical memory. Kvminit first allocates a page of physical memory to hold the root page-tablepage. kvminit通过kalloc，分配给kernel page table一个page，并且通过kvmmap将kernel的虚拟内存与物理内存一一对应起来。这里映射的主要是PHYSTOP之下的内存。 123456789101112131415161718192021222324252627voidkvminit(){ kernel_pagetable = (pagetable_t) kalloc(); memset(kernel_pagetable, 0, PGSIZE); // uart registers kvmmap(UART0, UART0, PGSIZE, PTE_R | PTE_W); // virtio mmio disk interface kvmmap(VIRTIO0, VIRTIO0, PGSIZE, PTE_R | PTE_W); // CLINT kvmmap(CLINT, CLINT, 0x10000, PTE_R | PTE_W); // PLIC kvmmap(PLIC, PLIC, 0x400000, PTE_R | PTE_W); // map kernel text executable and read-only. kvmmap(KERNBASE, KERNBASE, (uint64)etext-KERNBASE, PTE_R | PTE_X); kvmmap((uint64)etext, (uint64)etext, PHYSTOP-(uint64)etext, PTE_R | PTE_W); // map kernel data and the phy return 0; in the kernel. kvmmap(TRAMPOLINE, (uint64)trampoline, PGSIZE, PTE_R | PTE_X); // map kernel data and the physical RAM we'll make use of.} 我们proc的kernel和全局kernel这些初始化的东西是一模一样的，所以只需要改一改kvminit以及kvmmap这两个函数就行，这两个函数是直接用的vm.c中的全局变量kernel_pagetable，那我们的kvminit需要创建一个kernel_pagetalbe并将它返回，而我们的kvmmap则用传入的pagetable代替全局的kernel_pagetable。 123456789101112131415161718192021222324252627282930313233343536373839404142434445// kernel/vm.c// used for proc_kvminit// same as kvmmpvoidproc_kvmmap(pagetable_t proc_kernel_pagetable,uint64 va, uint64 pa, uint64 sz, int perm){ if(mappages(proc_kernel_pagetable, va, sz, pa, perm) != 0) panic(&quot;proc_kvmmap&quot;);}// used for porc to have a kernel pagetable// same as kvminitpagetable_t proc_kvminit(){ pagetable_t proc_kernel_pagetable = (pagetable_t) kalloc(); if(proc_kernel_pagetable == (pagetable_t)0) return 0; memset(proc_kernel_pagetable, 0, PGSIZE); // uart registers proc_kvmmap(proc_kernel_pagetable,UART0, UART0, PGSIZE, PTE_R | PTE_W); // virtio mmio disk interface proc_kvmmap(proc_kernel_pagetable, VIRTIO0, VIRTIO0, PGSIZE, PTE_R | PTE_W); // CLINT proc_kvmmap(proc_kernel_pagetable, CLINT, CLINT, 0x10000, PTE_R | PTE_W); // PLIC proc_kvmmap(proc_kernel_pagetable, PLIC, PLIC, 0x400000, PTE_R | PTE_W); // map kernel text executable and read-only. proc_kvmmap(proc_kernel_pagetable, KERNBASE, KERNBASE, (uint64)etext-KERNBASE, PTE_R | PTE_X); // map kernel data and the physical RAM we'll make use of. proc_kvmmap(proc_kernel_pagetable, (uint64)etext, (uint64)etext, PHYSTOP-(uint64)etext, PTE_R | PTE_W); // map the trampoline for trap entry/exit to // the highest virtual address in the kernel. proc_kvmmap(proc_kernel_pagetable, TRAMPOLINE, (uint64)trampoline, PGSIZE, PTE_R | PTE_X); return proc_kernel_pagetable;} 并且需要在 kernel/proc.c中的allocproc函数，调用proc_kvminit来初始化进程的kernel page table。 1234567p-&gt;kernel_pagetable = proc_kvminit();if(p-&gt;kernel_pagetable == 0){ freeproc(p); release(&amp;p-&gt;lock); return 0;} 接下来需要将进程本身映射到进程的kernel pagetable中的kstack，下面这段是xv6 book中对于kstack作用的描述，个人理解是kstack是保存进程在kernel mode 时产生的数据的， 而且需要用stap寄存器保存当前进程的stap。 Each process has two stacks: a user stack and a kernel stack ( p-&gt;kstack ). When the process isexecuting user instructions, only its user stack is in use, and its kernel stack is empty. When theprocess enters the kernel (for a system call or interrupt), the kernel code executes on the process’skernel stack; while a process is in the kernel, its user stack still contains saved data, but isn’t ac-tively used. A process’s thread alternates between actively using its user stack and its kernel stack.The kernel stack is separate (and protected from user code) so that the kernel can execute even if aprocess has wrecked its user stack. 在procinit中，是将申请内存之后的kernel stack的物理地址通过用KSTACK生成的虚拟地址，映射在kernel pagetable高地址处，并且给每个kstack配备了一个guard page（见上图）。 procinit (kernel/proc.c:26) , which is called from main , allocates a kernel stack for each pro-cess. It maps each stack at the virtual address generated by KSTACK , which leaves room for theinvalid stack-guard pages. kvmmap adds the mapping PTEs to the kernel page table, and the call tokvminithart reloads the kernel page table into satp so that the hardware knows about the newPTEs. 1234567891011121314151617181920212223voidprocinit(void){ struct proc *p; initlock(&amp;pid_lock, &quot;nextpid&quot;); for(p = proc; p &lt; &amp;proc[NPROC]; p++) { initlock(&amp;p-&gt;lock, &quot;proc&quot;); // Allocate a page for the process's kernel stack. // Map it high in memory, followed by an invalid // guard page. char *pa = kalloc(); if(pa == 0) panic(&quot;kalloc&quot;); uint64 va = KSTACK((int) (p - proc)); kvmmap(va, (uint64)pa, PGSIZE, PTE_R | PTE_W); p-&gt;kstack = va; p-&gt;kstack_pa = (uint64)pa;// 这是我后来添加的 } kvminithart();} 我们需要给proc自带的kernel pagetable实现kstack的映射，这里我给struct proc添加了一个变量，来存储在proinit初始化后产生的kstack的物理地址。 1uint64 kstack_pa; //kstack pa in kernel pagetable 并且给procinit添加一段保存的代码，这段代码我已经在上面给出来了。然后我们需要在allocproc中，进行映射。 1234567891011121314// Allocate a page for the process's own kernel stack(p-&gt;kernel_pagetable).// Map it high in memory, followed by an invalid// guard page.//initlock(&amp;p-&gt;lock, &quot;proc&quot;);uint64 va = p-&gt;kstack;uint64 pa = p-&gt;kstack_pa;if(pa == 0){ printf(&quot;%p&quot;, va); panic(&quot;allocproc: invalid pa\\n&quot;);}printf(&quot;%p\\n&quot;, pa);//va = KSTACK(0);proc_kvmmap(p-&gt;kernel_pagetable ,va, (uint64)pa, PGSIZE, PTE_R | PTE_W); 我们还需要修改scheduler， 使它每次选取一个proc运行的时候，修改satp，使satp指向该proc的kernel pagetable而不是全局的kernel pagetable。 123456789101112131415161718192021222324252627282930313233343536373839404142434445voidscheduler(void){ struct proc *p; struct cpu *c = mycpu(); c-&gt;proc = 0; for(;;){ // Avoid deadlock by ensuring that devices can interrupt. intr_on(); int found = 0; for(p = proc; p &lt; &amp;proc[NPROC]; p++) { acquire(&amp;p-&gt;lock); if(p-&gt;state == RUNNABLE) { // Switch to chosen process. It is the process's job // to release its lock and then reacquire it // before jumping back to us. p-&gt;state = RUNNING; c-&gt;proc = p; w_satp(MAKE_SATP(p-&gt;kernel_pagetable));//change the satp register to current process kernel pagetable sfence_vma(); swtch(&amp;c-&gt;context, &amp;p-&gt;context); kvminithart();// 当proc执行完之后，需要将全局kernel page table 转到 satp中 // Process is done running for now. // It should have changed its p-&gt;state before coming back. c-&gt;proc = 0; found = 1; } release(&amp;p-&gt;lock); }#if !defined (LAB_FS) if(found == 0) { intr_on(); kvminithart(); asm volatile(&quot;wfi&quot;); }#else ;#endif }} 完成这些之后，我们就要开始最后一步，当进程释放的时候，也要释放kernel page table 并且不能释放对应的物理内存(因为物理内存是各个kernel page table 共享的)，释放pagetable 用到了walkaddr，我们需要修改一下walkaddr，使其遇到叶子page的时候，不要调用painc，这样我们就达到了释放page table而不释放对应的物理内存。 123456789101112131415voidproc_freewalk(pagetable_t pagetable){ // there are 2^9 = 512 PTEs in a page table. for(int i = 0; i &lt; 512; i++){ pte_t pte = pagetable[i]; if((pte &amp; PTE_V) &amp;&amp; (pte &amp; (PTE_R|PTE_W|PTE_X)) == 0){ // this PTE points to a lower-level page table. uint64 child = PTE2PA(pte); proc_freewalk((pagetable_t)child); } } kfree((void*)pagetable);} 最后在 kernel/proc.c 中的free_proc中调用这个函数就行了。 12if(p-&gt;kernel_pagetable) proc_freewalk(p-&gt;kernel_pagetable); 最后的最后，我在这些步骤中省略了取defs.h中声明函数或这变量的过程，在做的时候这一步不要落下。这样我们的这一个实验就大功告成了。 Simplify copyin/copyinstr实验大意在上一个实验中，每个proc都有了自己的kernel pagetable，这个实验是将copyin以及copyinstr简化，copyin和copyinstr是先将传入的pagetable中va对应的pa解析出来， 然后将数据传入pa中。我们简化后的版本是不需要翻译这一步，可以直接将数据从源地址放到目标地址中。 过程我们需要将每个proc的user pagetable复制到kernel pagetable中，并且要将PTE中的PTE_U标记删除，因为在kernel mode中，无法访问带有PTE_U标记的PTE。实现这个函数可以照着 kernel/vm.c 中的uvmcopy来，不过不需要创建新的pte，只需要将kernel pagetable中的pte改为对应的user kernel pagetable中的pte即可。 12345678910111213141516171819202122232425//Given a process pagetable, copy it to the process's kernel pagetable//the copied memories page_start at va 0//remove flage PTE_U for kernel modeintu2kvmcopy(pagetable_t pagetable, pagetable_t proc_kernel_pagetable, uint64 page_start, uint64 page_end){ pte_t *ptefrom, *pteto; uint64 pa, i; uint flags; if(page_end &lt; page_start) return -1; page_start = PGROUNDUP(page_start); for(i = page_start; i &lt; page_end; i += PGSIZE){ if((ptefrom = walk(pagetable, i, 0)) == 0) panic(&quot;uvmcopy: pte should exist&quot;); if((pteto = walk(proc_kernel_pagetable, i, 1)) == 0) panic(&quot;uvmcopy: pte should exist&quot;); pa = PTE2PA(*ptefrom); flags = PTE_FLAGS(*ptefrom); flags = (flags &amp; (~PTE_U)); *pteto = (PA2PTE(pa) | flags);//将pte指向pa } return 0;} 然后就是修改fork(), exec(), sbrk()这些创建或更改user table的函数，使其调用上面的u2kvmcopy 12345if(u2kvmcopy(np-&gt;pagetable, np-&gt;kernel_pagetable, 0, np-&gt;sz) &lt; 0){ freeproc(np); release(&amp;np-&gt;lock); return -1;} fork和exec更改方式一样，就是将上面的代码添加进对应的位置就行。sbrk()有所不同，因为user pagetable的大小不能超过在kernel pagetable中的PLIC地址，所以需要加一个判断（sbrk是 kernel/sysproc.c中的sys_sbrk函数，并且调用的是 kernel/proc.c中的growproc函数）。 12345678910111213141516171819202122// kernel/proc.c// Grow or shrink user memory by n bytes.// Return 0 on success, -1 on failure.intgrowproc(int n){ uint sz; struct proc *p = myproc(); sz = p-&gt;sz; uint old_sz = sz; if(n &gt; 0){ if(sz + n &gt; PLIC || (sz = uvmalloc(p-&gt;pagetable, sz, sz + n)) == 0) { //如果增长后的地址大于PLIC，那么返回错误 return -1; } u2kvmcopy(p-&gt;pagetable, p-&gt;kernel_pagetable, old_sz, sz);//将新增的page复制给kernel_pagetable } else if(n &lt; 0){ sz = uvmdealloc(p-&gt;pagetable, sz, sz + n); } p-&gt;sz = sz; return 0;} 还需要修改kernel/proc.c 中的userinit函数，这个函数是第一个进程初始化的函数，也需要调用u2kvmcopy。 12345678910111213141516171819202122232425voiduserinit(void){ struct proc *p; p = allocproc(); initproc = p; // allocate one user page and copy init's instructions // and data into it. uvminit(p-&gt;pagetable, initcode, sizeof(initcode)); p-&gt;sz = PGSIZE; // prepare for the very first &quot;return&quot; from kernel to user. p-&gt;trapframe-&gt;epc = 0; // user program counter p-&gt;trapframe-&gt;sp = PGSIZE; // user stack pointer safestrcpy(p-&gt;name, &quot;initcode&quot;, sizeof(p-&gt;name)); p-&gt;cwd = namei(&quot;/&quot;); p-&gt;state = RUNNABLE; u2kvmcopy(p-&gt;pagetable, p-&gt;kernel_pagetable, 0, p-&gt;sz);//调用 release(&amp;p-&gt;lock); printf(&quot;%d\\n&quot;, p-&gt;lock);} 最后，需要将实验已经写好的 kernel/vmcopyin.c中的 copyin_new 以及 copyinstr_new放到 kernel/vm.c 中的 copyin以及 copyinstr中即可，还要记得将copyin_new和copyinstr_new放到defs.h中。 12345678910111213141516171819// kernel/vm.c// Copy from user to kernel.// Copy len bytes to dst from virtual address srcva in a given page table.// Return 0 on success, -1 on error.intcopyin(pagetable_t pagetable, char *dst, uint64 srcva, uint64 len){ return copyin_new(pagetable, dst, srcva, len);}// Copy a null-terminated string from user to kernel.// Copy bytes to dst from virtual address srcva in a given page table,// until a '\\0', or max.// Return 0 on success, -1 on error.intcopyinstr(pagetable_t pagetable, char *dst, uint64 srcva, uint64 max){ return copyinstr_new(pagetable, dst, srcva, max);} 实验就完成啦^_^可以看一下实现的copynew_in，可以发现，已经取消翻译的那一步了。 123456789101112131415// Copy from user to kernel.// Copy len bytes to dst from virtual address srcva in a given page table.// Return 0 on success, -1 on error.intcopyin_new(pagetable_t pagetable, char *dst, uint64 srcva, uint64 len){ struct proc *p = myproc(); if (srcva &gt;= p-&gt;sz || srcva+len &gt;= p-&gt;sz || srcva+len &lt; srcva) return -1; memmove((void *) dst, (void *)srcva, len); stats.ncopyin++; // XXX lock return 0;}","link":"/2021/10/02/mit-6-S081-Lab3-page-tables/"},{"title":"解决vscode中出现struct sigaction incomplete error","text":"今天在学sigaction的时候，vscode总是蹦出struct sigaction incomplete error，而且没有代码提示，很烦人，然后我在reddit中找到了个不是很优雅的解决方法。 问题 解决方法在c_cpp_properties.json文件的includePath中加入/usr/include/x86_64-linux-gnu/bits/sigaction.h，然后在对应c文件中加入#include&lt;sigaction.h&gt;头文件。 123456789101112131415161718{ &quot;configurations&quot;: [ { &quot;name&quot;: &quot;Linux&quot;, &quot;includePath&quot;: [ &quot;${workspaceFolder}/**&quot;, &quot;/usr/include/**&quot;, &quot;/usr/include/x86_64-linux-gnu/bits/sigaction.h&quot; ], &quot;defines&quot;: [], &quot;compilerPath&quot;: &quot;/usr/bin/clang-8&quot;, &quot;cStandard&quot;: &quot;c11&quot;, &quot;cppStandard&quot;: &quot;c++14&quot;, &quot;intelliSenseMode&quot;: &quot;linux-clang-x64&quot; } ], &quot;version&quot;: 4} 出现这样的问题貌似是因为sigaction没有定义在signal.h中，vscode没把它的头文件包含进来…方法来源：https://www.reddit.com/r/vscode/comments/9qfhr4/visual_studio_code_intellisense_missing/e972g7h/","link":"/2021/10/18/%E8%A7%A3%E5%86%B3vscode%E4%B8%AD%E5%87%BA%E7%8E%B0struct-sigaction-incomplete-error/"},{"title":"mit 6.S081 Lab4 traps","text":"本次实验是让我们探寻xv6如何实现trap，需要阅读xv6 book的第四章 RISC-V assembly占坑…这个我没做… Backtrace实验大意这个实验让做一个在调用sys_sleep之前输出调用栈信息的小程序。 过程在hint里面提供了一个极为抽象的图，不过很有用。这张图给出了xv6中栈帧的结构，hints里面提供了一个函数返回fp，fp是栈顶的地址，fp-8是返回地址，fp-16是存放指向上一个栈帧的fp的地址，这样我们所需要的元素都集齐了，我们获得一个fp之后，打印我们需要的信息，然后再找到指向上一个栈帧的fp的地址，解引用这个fp就可以获得一个栈帧的栈顶地址了，所以只需要在kernel/riscv.h添加r_fp函数以及在kernel/printf.c中添加backtrace即可。 12345678910111213141516// kernel/printf.cvoid backtrace(){Alarm uint64 fp = r_fp(); uint64 hight, low; hight = PGROUNDUP(fp);//我们需要找到当前页的最高地址和最低地址，用来判断找栈帧的过程是否结束了。 low = PGROUNDDOWN(fp); printf(&quot;backtrace:\\n&quot;); while(fp &gt;= low &amp;&amp; fp &lt; hight) { //uint64 *fp_pointer = (uint64 *)fp; printf(&quot;%p\\n&quot;, *(uint64 *)(fp - 8)); fp = (*(uint64 *)(fp - 16));//记得获取的是fp这个指针指向的地址，而不是fp本身的值。 }} 最后需要在kernel/sysproc.c的sys_sleep中调用backtrace 12345678910111213141516171819202122// kernel/sysproc.cuint64sys_sleep(void){ backtrace(); //调用backtrace int n; uint ticks0; if(argint(0, &amp;n) &lt; 0) return -1; acquire(&amp;tickslock); ticks0 = ticks; while(ticks - ticks0 &lt; n){ if(myproc()-&gt;killed){ release(&amp;tickslock); return -1; } sleep(&amp;ticks, &amp;tickslock); } release(&amp;tickslock); return 0;} Alarm这个实验我大概已经做完一个多月了… 实验大意大概意思是让实现一个time trap，当时间到的时候，调用回调函数。首先要在proc结构体中加入已用时间，和alarm传入的触发时间。 123456789101112131415161718192021222324252627282930struct proc { struct spinlock lock; // p-&gt;lock must be held when using these: enum procstate state; // Process state struct proc *parent; // Parent process void *chan; // If non-zero, sleeping on chan int killed; // If non-zero, have been killed int xstate; // Exit status to be returned to parent's wait int pid; // Process ID // these are private to the process, so p-&gt;lock need not be held. uint64 kstack; // Virtual address of kernel stack uint64 sz; // Size of process memory (bytes) pagetable_t pagetable; // User page table struct trapframe *trapframe; // data page for trampoline.S struct trapframe backup_trapframe; struct context context; // swtch() here to run process struct file *ofile[NOFILE]; // Open files struct inode *cwd; // Current directory char name[16]; // Process name (debugging) int siga_ticks;//zaizheli int ticks_sum; uint64 siga_handler; int ishandling;}; 然后去sysproc.c中完成alarm对应的系统函数 123456789101112uint64 sys_sigalarm(void){ int siga_ticks;//间隔多少时间片调用一次回调函数 uint64 siga_handler;//指向回调函数的指针 if(argint(0, &amp;siga_ticks) &lt; 0 || argaddr(1, &amp;siga_handler) &lt; 0) return -1; struct proc *p = myproc(); p-&gt;siga_handler = siga_handler; p-&gt;siga_ticks = siga_ticks; return 0;} 然后还需要去syscall.c中添加声明，这里就略过了，和之前操作是一样的。这样我们就完成了alarm的调用，以及进程信息中关于alarm的信息初始化。之后我们就需要去trap中添加相应的代码，本来是想将trap调用的过程写一遍的，先留坑把… 123456789// kernel/trap.c usertrapif(which_dev == 2)//判断时候因为alarm中断的 { if(p-&gt;siga_ticks != 0) { p-&gt;ticks_sum ++;//加一个时间片 } yield();//因为是每个时间片cpu中的进程都需要调度，所以yield } 当这个proc又被cpu执行的时候，会调用usertrapret函数，进行返回到用户态的准备，这里我们需要判断time是否到了，添加了一个ishandling标志位，标志是否这个进程时候就是在正在运行回调函数，如果是，那么就不要再次调用回调函数了。使用w_sepc将回调函数的地址赋值给pc，这样返回用户态的时候就会到达回调函数。backup_trapframe保存了当前的trapframe，用作回调函数结束后，返回原来调用前状态的备份。 12345678// kernel/trap.c usertrapretif(p-&gt;siga_ticks != 0 &amp;&amp; p-&gt;ishandling != 1&amp;&amp;p-&gt;ticks_sum == p-&gt;siga_ticks) { p-&gt;ticks_sum = 0; p-&gt;backup_trapframe = *(p-&gt;trapframe); p-&gt;ishandling = 1; w_sepc(p-&gt;siga_handler); } 这次写的比较乱…因为隔了很久很久了，最近在也准备实习之类的事情…","link":"/2021/10/25/mit-6-S081-Lab4-traps/"},{"title":"C++ string::earse的时间复杂度","text":"这里只看 1string&amp; erase (size_t pos = 0, size_t len = npos); 这个重载的代码 源代码直接看这个earse的源代码 1234567891011 basic_string&amp; erase(size_type __pos = 0, size_type __n = npos) {_M_check(__pos, &quot;basic_string::erase&quot;);if (__n == npos) this-&gt;_M_set_length(__pos);else if (__n != 0) this-&gt;_M_erase(__pos, _M_limit(__pos, __n));return *this; } 这部分代码调用了_M_erase 123456789101112template&lt;typename _CharT, typename _Traits, typename _Alloc&gt; void basic_string&lt;_CharT, _Traits, _Alloc&gt;:: _M_erase(size_type __pos, size_type __n) { const size_type __how_much = length() - __pos - __n; if (__how_much &amp;&amp; __n)this-&gt;_S_move(_M_data() + __pos, _M_data() + __pos + __n, __how_much); _M_set_length(length() - __n); } 可以看到，这里又调用了_S_move，输入的参数分别是要删除位置的指针和删除子串之后的指针。这里其实就是将后面的未删除的部分移动到删除的位置。 12345678static void _S_move(_CharT* __d, const _CharT* __s, size_type __n) { if (__n == 1) traits_type::assign(*__d, *__s); else traits_type::move(__d, __s, __n); } 可以看到这里用了move，将s指针的字符串移动到d指针的位置，移动的大小是n。时间复杂度为$O(N)$不过我不是很能理解，将s位置的n个字符移动到d之后，那么s+n的字符怎么办？有待查证….","link":"/2021/11/13/C-string-earse%E7%9A%84%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6/"},{"title":"exit和_exit区别","text":"_exit不会刷新标准输出，而exit会 _exit不能用来清理进程，而exit可以用来注册在on_exit或者at_exit上用来清理准备退出的进程。 个人理解_exit仅仅是退出当前的进程，而exit是退出进程的同时，清理进程的残余物。 参考https://stackoverflow.com/questions/5422831/what-is-the-difference-between-using-exit-exit-in-a- conventional-linux-fo","link":"/2022/01/02/exit%E5%92%8C-exit%E5%8C%BA%E5%88%AB/"},{"title":"CMU-15-445笔记(一)","text":"DISK-ORIENTED DBMS数据库在内存中设立了buffer pool来缓存外存中的信息 为什么不使用OS的虚拟内存？这里主要原因是数据库需要足够的权限来控制页的置换 数据库信息在磁盘中的表示数据库存储数据是使用了一到多个文件在磁盘上 DATABASE PAGES一个Page是固定大小的，每个页都有一个ID（不同），作用是数据库取页的时候会直接通过页ID来取，然后再通过间接层去寻找在物理层面上的页，这样使物理层面的页与数据库层面的页放置位置解耦 页的种类 HardWare Page 通常4KB OS Page 通常4KB Database Page 512B-16KB页在磁盘上的形式 堆形式 连续 哈希heap fileheap file 形式在磁盘中是无序的，所以需要元信息去寻找页在磁盘中位置，有两种方式 Linked List Page DirectoryLinked ListLinked List是有一个头结点，头结点分别指向free page list和data page listPage Directory使用一个Directory，哈希每个页的位置Page的结构Slotted PagesSlotted Page头部有slot数组，每个slot指向一个页底层存储的tuple（实际上记录的是tuple在页中的offset），slot数组是向页底层增长，而tuple是从底层向上增长，直到这俩中间空间不够，页就满了Log-Structed File Organization在这种形式中，页不再存储实际的数据，而只记录操作日志数据库会将修改数据库数据的log插入到page当中如果要读取这些记录，数据库会从后往前扫也可以使用索引，直接跳转到想要的记录的位置上Tuple Data每个Tuple有Header，记录了一些元信息 并发控制的粒度 Bit Map for NULL values每个Tuple可以认为是每个表中的行数据，Tuple的结构和定义表是的结构一致","link":"/2022/04/12/CMU-15-445%E7%AC%94%E8%AE%B0-%E4%B8%80/"},{"title":"CMU-15-445笔记(二)","text":"占用空间大的值在tuple中的存储形式Overflow storage pages如果c是较大的值的话，c的位置可以放置指针，指向一个格外的页存储真实的数据，如果数据过大，可以在一个overflow page中再指向下一个overflow page。overflow page是在内存中常驻的，并且overflow page对于用户层面是透明的。 常见的overflow page名称与大小 Postgres: TOAST(&gt; 2KB) MySQL: Overflow(&gt; 1/2 size of page) SQL Server: Overflow(&gt; size of page)External Value Storage这种形式是将较大的数据放在数据库外部，当做一个常规文件，c可以是指针或者路径名称。数据库不能控制外部文件的内容，也就没有了持久和事务的保护System Catalogs数据库都会将他们数据的catalog存入数据库内部catalog主要有 Table, columns, indexes, views Users, permissions Internal statistics它们本身会被存为table，tuple。数据库的存储模型OLTP主要包含简单的读写语句，每个语句都只操作数据库的一小部分数据OLAP主要处理复杂的，需要检索大量数据并聚合的操作Data Storage Models关系数据库将数据attributes组合成tuple，将结构相似的tuple组合成relation，它没有指定这些relation中的tuple以及tuple的attribute的存储方式。一个tuple的所有attributes并不需要存储在同一个页中常见的Data Stroage Models有 行存储 N-ary Storage Model(NSM) 列存储 decomposition Storage Model(DSM)NSMNSM将一个tuple的所有attributes在页中连续的存储，这种存储方式适合OLTP场景DBMS可以将一些常用的attribute建立index，查询语句就可以通过index找到相应的tuples对于只需要部分attribute的操作来说，这种方式会导致页中所有的数据被读入内存，造成空间上的浪费这里只需要读入最后两列，但是NSM需要将所有页读入内存NSM优缺点优点： 快速的插入，更新，删除 对于需要整个tuple是一个很好的选择缺点：对于需要大量查询范围，并且仅需要tuple中少数的attributes会造成空间上的浪费DSMDSM是将列属性放在一个页中这样就可以很好的解决NSM的问题需要哪列直接读取即可，没有读取无用列的浪费 Tuple Identification同时这也带出来了一个问题，怎么匹配每个列属性Offsets最常用的方法，页中的值都有同样的长度，这样就可以直接使用offset来判断对应的行是哪个Embefdded Tuple Ids给每个attribute添加行id，浪费空间DSM优缺点优点： 减少了NSM中冗余的IO，DSM只读取它需要的数据 更好的处理查询以及更好的数据压缩支持缺点： 涉及少量tuples，多数attributes的查询低效","link":"/2022/04/13/CMU-15-445-%E7%AC%94%E8%AE%B0%E4%BA%8C/"},{"title":"CMU-15-445笔记(三)","text":"Buffer Pool OrganizationBuffer Pool在内存中是以一个固定大小数组的形式来呈现的，数组中每个元素都有固定的大小，称为frame当DBMS需要一个page的时候，DBMS可以让硬盘中的page转存到Buffer Pool中来 Page TablePage Table建立起 page id 到 frame id的映射，是一个哈希表同时page table也会设立起标志位，表明当前page的状态 Dirty Flag Pin/Reference Counter 主要用来避免Buffer Pool替换掉对应的page，因为此刻可能多个线程在共同访问这个页当page不在page table中的时候，DBMS会申请一个latch，给对应的entry上锁，然后从disk读取相关的page到buffer pool，然后释放latch Locks &amp;&amp; LatchesLocks锁在数据库中是一个高层次概念的东西，例如查询的时候会上锁，此刻锁是一种概念，例如next-key lock锁的主要作用有 保护事务 在事务期间持有 LatchesLatches是一个低级的原语，类似OS中的锁 用来保护数据库中的临界区 只在数据库进行操作的时候持有 Memory ManagementMultiple Buffer Pools数据库中可以有多个Buffer Pool，每个Buffer Pool都可以有不同的缓冲策略，这样的好处一个是可以优化特定种类的查询，另一个是减少latch的使用，提高并发。 实现Object Id将record id和他们对应的page id，slotnum存在一起，从而让只当的record id始终映射到特定的buffer pool中 Hashing直接hash record id，然后去查找对应的buffer pool，然后映射 Pre-FetchingPre fetching可以当Buffer Pool获取一个page的时候，其他相关联（在磁盘中连续的，或者是page id连续的）的page也预缓存到Buffer Pool中 Scan Sharing当多个线程查询有重叠的时候，例如A先查询，B发现A的查询可以被自己使用的时候，就会共用A的，等A扫描完后，再去扫描剩余自己需要的数据。 Buffer Pool Bypass连续的扫描操作不会存到buffer pool，防止buffer pool溢出（来回交换page OS Page Cache大多数数据库不会让OS页缓存数据，所以可以直接使用O_DIRECT来告诉操作系统不要缓存 Buffer Pepacement PoliciesLRU替换最久未使用的页 Clockclock是LRU的替换策略，因为LRU开销略大，clock是给每个页设置个reference bit，当page被访问的时候 reference bit设置为1，当要移除page的时候，从上一个访问的位置开始，轮询访问每个page的reference bit，如果bit为1，则重置为0，若为0，则移除page LRU和Clock的问题这俩的一个问题是sequential flooding 当查询是连续的查询的时候，会读取每个页 这样就会污染buffer pool，又可能新读取的page再也不会读取第二次 LRU-K记录最后K次的访问时间戳，然后计算每次访问的时间间隔，从而DBMS可以知道下一次哪个页最可能被访问 LocalizationDBMS针对每个查询都做出pages的限制，使得对buffer pool的影响降低到最小 Priority HintsDBMS获取每个页的在查询过程中的上下文，从而得知一个页是否重要 Dirty Pages如果一个页时脏的（被修改过），那么DBMS必须将页的内容写回硬盘中，所以移除Dirty Page开销相对来说是较大的。 Background WritingDBMS可以周期性的将脏页写回硬盘中，写回需要在log记录之后再写回。","link":"/2022/04/14/CMU-15-445%E7%AC%94%E8%AE%B0-%E4%B8%89/"},{"title":"CMU-15-445 Lab1记录","text":"LRU REPLACEMENT POLICY这个task是做lru替换类，负责跟踪buffer pool中的page 思路 LRU的思可以参考leetcode中的lru的实现，双向链表＋map，实现增删改查都是O(1)的操作 其他的跟着TASK1中的描述就可以了 Task 2 BUFFER POOL MANAGER INSTANCE这个task 需要是实现一个buffer pool manager(BufferPoolManagerInstance)，BufferPoolManagerInstance是负责将database page从DiskManager取出来并将它们放到内存中的一个类。BufferPoolManagerInstance也可以将脏页写回到磁盘以此来给page腾出地方。在内存中的pges都是由Page类来管理的，每个Page都包含DiskManager从物理磁盘中读取出来的一块大小的数据。BufferPoolManagerInstance会重复使用Page对象，所以同一个Page可能会包含不同的内容。Page由page_id来表明包含的物理页是哪个。如果Page对象没有包含一个物理页，就将其page_id设置为IINVALID_PAGE_ID 每一个Page对象都包含一个计数器，表明有多少个线程Pinned了这个page。所以BufferPoolManagerInstance是不允许free一个被Pinned的Page。每个Page还表明当前的页是否是Dirty的。BufferPoolManagerInstance当要重用包含脏页的Page的时候，必须先写回。BufferPoolManagerInstance需要使用刚才实现的LRUReplacer。 思路 FechPgImg(page_id) 这个函数基本跟着cpp文件中的提示来就行，如果在replacer获取到的page是脏页，需要flush一下，并且记得更新page的page_id,pin_count等参数 UnpinPgImg(page_id, is_dirty) 先执行pin_count–的操作，还要更新is_dirty_，最后如果pin_count为0的话，调用replacer-&gt;Unpin(frame_id)，将page调回replacer中 FlushPgImg(page_id) 调用disk_manager-&gt;WritePage即可 FlushAllPagesImpl() 同上 NewPgImp(page_id) 跟着提示来，如果获取的页是dirty的，就flush DeletePgImg(page_id) 跟着提示来，如果脏页就刷新，如果确定要删除这个页，需要调用replacer-&gt;Pin(frame_id)，将这个page对应的frame删除，再放入free_list中，否则会造成replacer和free_list中同时存在的情况 Task 3 PARALLEL BUFFER POOL MANAGER这个task需要将buffer pool实现为线程安全。ParallelBufferPoolManager是一个有多个BufferPoolManagerInstance的类，每个操作都会让ParallelBufferPoolManager选择一个BufferPoolManagerInstance进行操作。当给出page id的时候，需要通过取模的方式选择一个BufferPoolManagerInstance，方法是page_id mod num_instances。 思路首先个刚才实现的BUFFER POOL MANAGER INSTANCE加锁，可以直接使用它自身带的latch_，加锁方式可以使用c++17中的scoped_lock。剩下的就没啥可注意的了","link":"/2022/04/15/CMU-15-445-Lab1%E8%AE%B0%E5%BD%95/"},{"title":"CMU-15-445-Lab2记录","text":"TASK1实现HashTableDirectoryPage和HashTableBucketPage两个类 实现方法这两个实现起来没有太多的难点 HashTableDirectoryPage其中GetLocalHighBit是用来获取 local_depths对应的位的，这个函数主要是用来获取要分裂/合并页的pair页其他函数就照着函数名来就行 HashTableBucketPage这个类中，occupied_和readalbe_数组都是位图，所以当想法访问第i位的时候，需要处理为 123uint32_t bit_index = bucket_idx / 8;uint32_t bias = bucket_idx % 8;return readable_[bit_index] | (1 &lt;&lt; bias); 其他的也没有太多好说的，按照提示来就行 TASK2 HASH TABLE IMPLEMNETATIONTask2是要实现一个extendible hasing scheme的hash table，需要支持Insert，点搜索GetValue和删除Remove。在本次task中只需要完成Inset,GetValue,Remove就可以了，不要改动VerifyIntegrityhash table需要要支持uni que和non-unique key。但是相同的value和key是不允许的。例如(key_0, value_0)与(key_0, value_1)是可以存在一个hash table中，但是(key_0, value_0)与(key_1, value_1)是不行的，如果出现这种情况，Insert需要返回false 实现细节 Directory Indexing 当插入到hash table的时候，需要使用lest-significant bits来索引directory。也可以使用most-significant bits，但是前者会简单一些 Splitting Buckets 当insert没有空间的时候，需要split一个bucket。比较推荐的方法是当insertion会导致一个page overflow的时候，再分裂。 Merging Buckets 当一个bucket空的时候必须进行Merging，为了让merging简化，task提供了一些规则： 只有当buckets空的时候才merge buckets只能和它localdepth相同的合并 buckets的depth为0的时候不能合并 实现方法InsertInsert是最复杂的函数 如果要插入的page没有满，那么就直接调用bucket_page的insert插入即可 如果满了且该页的depth和global depth相等，就说明directory 需要expand，expand的具体方法就是遍历directory_page的每个bucket，找出对应的pair bucket，然后让pair bucket指向相同的page id 即可 如果depth小于global depth，就不需要扩展 接下来是split bucket 大概思路是先将即将split的buket的数据放到一个vecotr里面，然后通过buffer_pool_manager_-&gt;NewPage获取新的页，最后将vector的数据分别放到这两个页里就行(注意之前的bucket的数据需要清空一下) RemoveRemove较为简单，先调用bucket_page的Remove函数，然后判断这个page是否为空，如果为空了，就开始合并page合并page首先将该bucket page的local_depth减一，然后删除掉对应的page，然后获取它的pair page，将pair page的对应page_id赋值给待merge的bucket_page，这样就完成了合并，最后遍历与这俩相关的bucket，将它们的page_id都赋值为刚刚的pair page的page_id GetValue直接调用bucket_page的GetValue就行了 TASK3 CONCURRENCY CONTROL实现并发 Insert先获取table_latch_的读锁，然后再获取bucket_page的写锁，判断bucket_page是否满，如果满了，释放table_latch_和bucket_page的锁，再获取table_latch_的写锁，和bucket_page的写锁，执行SplitInsert，并且重复执行，直到成功插入，如果执行SplitInset的时候，发现bucket_page未满（因为可能另一个线程删除了数据），直接执行bucket_page的inset就行了。 Remove这里我的实现直接上了table_latch_的写锁，后续需要改进 GetValue直接上table_latch_的读锁即可","link":"/2022/04/18/CMU-15-445-Lab2%E8%AE%B0%E5%BD%95/"},{"title":"CMU-15-445-笔记(四)","text":"IndexHash TableHash Function CRC-64 MurmurHash Google CityHash Facebook XXHash 一般用这个 Google FarmHash Static Hasing SchemesLinear Probe Hasing就是线性探测法 Non-Unique-Keys键可能是重复的情况 Separate Linked List将值存在列表中 Redundany Keys直接把键存入hash table中 Robin Hood HashingRobin Hood Hashing 会记录每个key在hash table中距离它最优位置的距离。当一个key插入的时候，会比较插入位置的key距离最优位置和即将插入key距离最优位置的大小，如果后者大，则交换两者的位置。 Cuckoo Hashing使用多个hash table，并且每个hash table都有不同的seed。 当插入一个键的时候，检查所有的table然后选择一个hash后的空槽。 如果没有空槽，那么就删掉一个元素，然后给这个元素rehash一下找新的位置 Dynamic Hash TablesChained Hashing如果hash到同一个桶，那就用链表连接下去 Extendible HashingExtendible Hasing不同的hash值可以公用一个页，当页满的时候，就会扩hash值，重新rehash，寻找一个值的时候，会去对应的页线性查找 Linear Hashinghash table维护一个分裂指针，表示当前指针以上都进行了分裂，需要进行二次hash才能获取其真正位置。当一个页overflows的时候，会在页后追加页，并且分裂当前分裂指针指向的桶，指针下移","link":"/2022/04/17/CMU-15-445-%E7%AC%94%E8%AE%B0-%E5%9B%9B/"},{"title":"CMU-15-445笔记(五)","text":"B+ TreeB+树是一个自平衡的树结构，查找，插入，删除都是O(log n)，B+是是一个M-way的搜索树 所有叶子结点的深度都是一样的 所有结点（除了root），都是至少半满的 M/2 &lt;= keys &lt;= M-1 所有内部结点都有k个keys以及k+1个非空子节点 节点叶子节点叶子节点内部存储类似page的存储，头部为元信息，然后存储key，最后存储value，key提供对应value的偏移值。叶子节点中存储value的方式主要有两种 Record Ids存储record id，指向数据存放的位置 Tuple Data直接将数据存放在leaf node中 B Tree和B+ Tree的区别B Tree中内部节点会存储value，但是B+ Tree中 value只会存在leaf node 中，所以B Tree中不会出现相同的key，但是B+ Tree中会。 Insert 首先找到要插入的叶子节点L 将数据按顺序放到L中 如果L空间足够，就完成插入了 如果不够，分裂L，成为L和L2，同时要在父节点上增加entry，如果父节点不够，就递归分裂 Delete 首先找到目标所在的叶子节点L，删除该数的entry 如果L至少半满，那么操作结束，如果少于半满，那么尝试从兄弟节点拆借entries，如果失败，那么就尝试与兄弟节点合并 如果与兄弟节点合并，那么递归的删除父节点的L的entry Clustered Indexes 聚簇索引聚簇索引是将主键按照顺序排序存储，每个table只能还有一个聚簇索引，如果table中没有主键，那么就设置一列隐藏列作为主键 Selection ConditionsB+树支持多种的搜索方式，以&lt;A, B, C&gt;为例 (a = 5 AND b = 3) (b = 3) B+ Tree Design ChoicesNode Size存储设备越小，Node Size应该越大 HDD 1MB SSD 10KB In-Memory 512B Merge Threshold可以延迟Merge操作，因为Merge操作开销过大 Variable Length Keys应对变长变量的方式 存储指针 现在已经不使用这种方法了 可变长度的节点 每个节点的大小可以改变，但是需要注意内存管理 这种方式也没人使用 Padding 固定一个最大大小的尺寸，存储后的数据小于这个尺寸，那么就填充空值 Key Map / Indirection 设立一个Key Map，用来指向key value的位置 Non-Unique Indexes使用索引的key可能是不唯一的，通常有两种方式 Duplicate Key 多次存储相同的key Value list Key只出现一次，但是需要维护一个key链表，表示在这个key下的value Intra-Node Search查找内部节点的方法： Linear 线性查找 Binary 二分查找 Interpolation 如果提前知道key的样子/属性/分布，可以用数学的方式来近似的查找位置 OptimizationsPrefix Compression如果存储在同一个叶子节点的keys 有相同的前缀，可以先将相同的前缀存储起来，然后存key的时候就不用存前缀了 Suffix Truncation在内部节点的key只用于查找子节点的位置，所以可以只存储足够的prefix就要可以达到这样的目的。 Bulk Insert如果提前有了所有的keys，那么可以排序，排完序之后从下往上建树，这是最快的建树方式 Pointer SwizzlingNode使用page id来存储对其他node的引用，所以DBMS需要每次都从buffer pool中获取对应的page，所以如果一个page已经pinned了，可以直接存储这个page的指针，避免再次去buffer pool 中获取page","link":"/2022/04/22/CMU-15-445%E7%AC%94%E8%AE%B0-%E4%BA%94/"},{"title":"CMU-15-445笔记(六)","text":"Duplicate Keys Append Record Id 将tuple的record id作为key的一部分，从而保证每个键都是不同的 Overflow Leaf Nodes 叶子节点分裂，将duplicate key存在overflow leaf node中，类似于链表的存储方式 Implicit Indexes数据库会给完整性约束创建对应的索引，但是对于引用约束（外键）来说不会 Partial Indexes局部索引可以减少index本身的大小 Covering Indexes覆盖索引，如果query所需要的数据都在index中，DBMS可以直接从index中获取查询结构，不用去获取tuple本身 Functional/Expression Indexes可以建立通过计算得到值的index 12CREATE INDEX idx_user_login ON users (EXTRACT(dow FROM login)); Trie IndexTrie树存储了数据的前缀，相同的前缀一个路径 Trie Key SpanSpan是key对应的进制编码1-bit Span Trie是指的存储二进制的key，每个node的分支树就是2 horizontal compreesion当为1-bit Span Trie的时候，可以不存储0，1，直接存储存储两个指针就可以，这样也可以隐式的来表示01。 vertical compreesion当前路径如果确定了一个特定的key的时候，可以提前终止路径，这样可以减少存储量，这样的trie 树也可以称之为Radix或者Patricia Tree红圈处被直接压缩了 Inverted IndexInverted Index用来存储文字与数据的对应关系，Inverted Index又可以叫做 full-text search index，也可以叫做concordance","link":"/2022/04/26/CMU-15-445%E7%AC%94%E8%AE%B0-%E5%85%AD/"},{"title":"CMU-15-445笔记(十)","text":"Processing ModelsDBMS的processing modesl定义了系统如何取执行一个query plan Interator Model又叫做volcano(火山)模型或者pipeline模型，是最常用的model，每个query plan操作都会实现一个Next function 对于每一个调用，操作符就会返回一个tuple或者如果不没有tuple的话，返回null 操作符实现了一个循环来调用它下一个子节点来遍历所有的tuples然后处理它们。相当是一个链式的调用来获取要处理的tuple 以hash join为例子 首先顶部调用child.next()来获取join好的tuple，调用执行到了(2)，(2)会遍历left的tuple，也就是(3)所返回的tuple，进行hash构建，然后同样遍历right的tuple并判断是否要join，执行了(4)(5)，返回tuple给(2)，最后(2)的tuple返回给(1)需要注意的是所有的next函数都是一次一次执行，不是将所有的结果全部返回。 Materialization Model 每个operator会一次性的处理它的input，然后一次性的发送它的output。在oltp中这个model很实用，因为queries只会访问少量大的tuple，但是olap就不适用，它会需要大量的中间结果. Vectorization Modelnext返回的不是一个tuple而是一批tuples Plan Processing Direction上面的方式都是top to bottom的方式来调用执行query plan的，其实还有bottom to top（这种方法需要所有的数据都在内存中） Access Methods访问数据有两种方式，一个是根据索引来访问，这个的优先级最高，但是不一定有，备胎选项就是顺序扫描table来获取数据 Sequential ScanDBMS会保存一个内部的cursor来追踪上一个page的位置，方便下一次被next调用后找到要返回的page Optimization Perferching Buffer Pool Bypass Parallelization Zone Maps 提前计算page中的聚合的attribute value，DBMS检查Zone Map来判断是否要访问这个page 比如以上这个，根据Zone Map中的Max就可以得知不需要去访问这个page Late Materialization 在列式存储中，获取一个tuple的数据是需要多次读取操作的，所以可以推迟读取操作 DBMS将查询操延迟，比如a&gt;100的操作，结果向操作树传递结果的时候，只传递tuple的offset，同理join操作，等真正需要这些数据的时候，再根据offset来获取数据。 Heap Clustering 直接用index来扫描，因为tuple根据index的书顺序来存储，所以是要给顺序IO Index ScanDBMS会挑选最合适的index进行查找，具体会在下一节中讲解 Multi-Index Scan如果多个index效果都差不多，可以使用多个索引共同查找 获取每个匹配的index扫描结果的Record IDs的集合 然后将这些集合根据query的条件来combine在一起 遍历这些records，然后根据query条件来获取最终结果 Index Scan Page Sorting如果直接用非聚簇索引叶子节点的顺序来访问每个page的话，是低效的，因为它们对应的tuple可能不是顺序的，所以如果结果并不需要一定是和叶子节点顺序是一样的，DBMS就可以先扫一遍叶子节点，获取全部的page id后，排序page id，然后再进行访问，这样就可以是顺序IOBy the way，聚簇索引维护tuple顺序的方式很粗暴，就是重排一遍。 Expression EvaluationDBMS会将where子句表示为一个表达式树节点是不同的表达式类型 比较(= &lt; &gt; ! =) AND，OR 计算符 常量 Tuple的属性以这个为例，每次判断都会从根节点出发，深度遍历，然后最后回到根节点来判断是否为true这种方式是比较慢的，因为每次判断都需要遍历整个表达式树，对于一些高级数据库来说，它们会直接使用JIT来编译判断条件，从而更好的执行。","link":"/2022/05/14/CMU-15-445%E7%AC%94%E8%AE%B0-%E5%8D%81/"},{"title":"CMU-15-445笔记(十一)","text":"Parallel DBMS与Distributed DBMS的区别 Parallel DBMS 资源是在物理层面上紧密相邻的 通过cpu总线/或者其他的内部的高速通道来互相通信 不同cpu的通信是可靠并且简单的 Distributed DBMS 资源可以离的很远，比如一个在北京，另一个在上海 通过网络或者其他较慢的传输通道 通信通常会到来不可忽视的花费和问题 Process Model定义了DBMS如何处理来自多个用户的并发请求，其中一个woker（可以是进程或者线程）是DBMS的某个组件，负责代表客户端 执行任务并返回结果 Process Pre Woker每个woker都是一个进程，由OS调度 woker使用共享内存来共享所需要的page 如果一个woker 崩溃了也不会影响整个数据库的运行 Process Pool用进程池来执行，避免了每次创建进程的开销 Thread Per Worker每个线程负责一个执行，开销更小，并且不用去管共享内存之类的事情，但是如果一个线程崩溃了，又可能会导致整个系统崩溃。多线程处理目前是用的最广的Process Model Scheduling由于DBMS了解本身要比OS多得多，所以基本所有的query plan都是由DBMS调度的。 Inter VS Intra Quera ParallelismInter Quera Parallelism是不同的queries并行执行，Intra Quera Parallelism是一个query的operations 并行执行 Inter Query Parallelism对于只读的queries来说，实现起来比较简单，但是一旦有写入的操作，就会发生冲突 Intra Query Parallelism多个works执行一个query，可以将其中的operators看作是一个生产者和消费者模型。例如在hash join中，可以将每个bucket的join操作分解给不同的wokers去做 Inter-Operator(Horizontal)将完整的一个操作分解进行操作，然后用exchange operator来将处理完成的结果结合起来传给操作流的下一个操作exchange operator并行的执行next操作来获取每个worker的结果一个更复杂的例子 Inter-Operator(Vertical)不同的线程来执行不同的operator，可以是每个operator对应不同的worker Bushy Parallelism这个相当于执行query的不同部分，这个部分可以是多个operators I/O Parallelism将DBMS分散放置到不同的devices中，这样达到一个并行的效果，减少disk对整个数据库速度的拖累 Mutil Disk Parallelism将DBMS的文件放到不同的storage devices中，例如可以是RAID Database Partitioning有些DBMS可以允许用户指定不同数据库存放的位置。 Parationing将一些logical table分开物理存储，理想情况下parationing应该对应用透明 Vertical Partitioning将table的attributes分开存储，类似列式存储数据库 Horizontal Partitioning通过设定partitioning key将table不同的segments分割开来","link":"/2022/05/15/CMU-15-445%E7%AC%94%E8%AE%B0-%E5%8D%81%E4%B8%80/"},{"title":"CMU-15-445笔记(十三)","text":"Cost ModelStatisticsDBMS将内部的一些关于table, attributes, indexs的静态信息存到内部的catalog里面，不同的系统会有不同的更新方式 Selection cardinality（选择基数）现在假设有R种关系，对于每个都有$N_R$个tuple，$V(A,R)$代表了$A$ attribute中不同的value的数量。假设数据分布出现的均匀，代表A属性下每个值平均的记录个数。$$SC(A,R)=N_R/V(A,R)$$ Complex Predicatesselectivity(选择率)表明table中有多少tuple符合查询的条件。每种操作都有不同的选择率的计算 Equality Predicate$$sel(A=constant) = SC(P)/N_R$$假设有一个age表，有0-4值，每个值出现的次数一样，那么$$SC(age=2)=1$$$$sel(age=2)=1/5$$ Range Predicate$$sel(A&gt;=a)=(A_{max}-a)/(A_{max}-A{min})$$例如$$sel(age&gt;=2)\\approx (4-2)/(4-0) \\approx 1/2$$这里的选择率和实际上的概率（3/5）是不等于的，有可能会导致operator拿到的数据比预估的多 Negation Query$$sel(not P) = 1-sel(P)$$例如$$sel(age!=2) = 1 (1-(1/5))=4/5$$这里selectivity = probability Conjunction$$sel(P1\\land P2)=sel(P1) \\bullet sel(P2)$$例如$$sel(age=2 \\land name ; LIKE ‘A%‘)$$这里假设两个概率都是独立的。 Disjunction$$sel(P1 \\vee P2) = sel(P1)+sel(P2) - sel(P1) \\bullet sel(P2)$$ 问题以上的计算都是基于三个假设 数据分布均匀 概率独立 join时每个tuple都有对应的outer tuple现实中三种假设不一定成立，所以会出现误差。假设现在要选择车的品牌和车的型号，因为上面的概率独立假设，品牌和型号的概率是要进行相乘的，但是实际上，这两个概率并不独立，型号对应了特定的品牌，所以就会出现预测的选择率偏低的情况。 统计方式对于不是均匀分布的数据来说，可以将每个attribute下的数据分为几个bucket，每个bucket，一种方式是每个bucket的宽度相同，但是这样对于bucket内小数据不公平另一种方式就是让每个bucket的总和大致相同 Sampling在高端数据库中，可以使用采样，将真实的table的分布采样成更小的副本，计算选择率之类的数据的时候直接用副本来计算。 Query Optimization有了以上的cost的计算方式，就可以用来选取 query plan Single Relation Query Plan对于单一关系的查询，首先是确定访问的方式 顺序查询 二分/聚簇查询 索引查询然后去判断条件的顺序，如果某个条件能去掉更多的数据，那它应该优先使用 OLTP Query Plan首先判断这个查询是否是sargable的（Search Argument Able ），大概意思就是查询过程可以使用一个最优(选择率)的索引来辅助查询。 Multi Relation Query Plan在System R系统中（早期系统），多重join使用的是 left deep join trees，这样可以使join后的结果可以直接通过pipelin传递，不需要中间存储来等待另一个join对于怎么实现join，需要考虑到join的顺序，operator的种类，以及访问table的方式，可以使用dynamic programming 来减少总的耗时。首先计算出所有的可能方式的花费，然后找一条总cost最少的路径来实现join 实现join的总综合步骤 枚举realtion的顺序，对于systemR来说，只考虑left outer join，所以最右两个被排除。 对于每个顺序（不包括被排除的），计算每种join algorithm的cost 计算每种访问table的cost Postgers Optimizer对于有12表join以下的，Postgers用的就是SystemR的方式，以上的用的是遗传算法。","link":"/2022/05/21/CMU-15-445%E7%AC%94%E8%AE%B0-%E5%8D%81%E4%B8%89/"},{"title":"CMU-15-445笔记(十二)","text":"Query OptimizationPipleline 首先用户写的SQL query经过SQL Rewriter重写优化（这个模块实现的比较少） 经过语法解析器分解成抽象语法树传给Binder Binder通过查询System Catalog将SQL查询中的命名对象转换为内部的标识符 Tree Rewirer有静态的规则，将语法树进行优化 然后传入优化器中，优化器会有一些动态的优化，可以进行cost的比较，从而选取最优的方案 最后形成Physical Plan 执行 Relational Algebra(代数) Equivalences(等价)如果两个关系代数产生的tuples set（顺序任意）是一样的，那么这两个关系代数就是等价的，所以query rewriting就是寻找一个高效的，等价关系代数替代用户所写的关系代数。 Selections对于Selections来说 越早执行filters越好 将复杂的predicate push down（大概意思就是将它往语法树下方一移动） JoinJoin是符合交换律和结合律的n-way join不同的顺序有 $4^n$种， 也就是卡特兰数 ProjectionsProjections越早执行越好，这样可以减少无用信息的来回复制，Projections也可以先将除了所需数据之外的数据丢弃。","link":"/2022/05/16/CMU-15-445%E7%AC%94%E8%AE%B0-%E5%8D%81%E4%BA%8C/"},{"title":"CMU-15-445笔记(十五) 基于锁的协议","text":"本次笔记来自《数据库系统概念》第七版 第18章的第一节 并发控制为了确保书屋的隔离性，DBMS采用的是并发控制来实现的，主要有两阶段封锁(two-phase locking)和快照隔离(snapshot isolation) 基于锁的协议可以通过以互斥的方式来访问数据项来实现隔离性，就让事务持有数据项的锁来实现这一功能。 锁每个事务需要根据对数据项Q指定的操作申请适当的锁。事务会将锁请求发送给并发控制管理器，事务只有被并发控制管理器授予锁之后才能继续对事务进行操作。 相容函数如果事务在已经有其他事务获取锁的数据项上再获取锁，就说明这两把锁是相容的。也就是comp(A,B)=true 封锁协议封锁协议是一组规则，规定了事务何时可以对每个数据项进行加锁和解锁。假设$T_i$再$Q$上持有A模式锁，后来$T_j$再$Q$上持有B模式锁，并且comp(A,B)=false，那么称作在调度$S$中，$T_i$先于$T_j$，记作$T_i\\rightarrow T_j$，这种表示形式类似优先图。如果调度$S$是一组遵守封锁协议规则的事务的一种可能的调度，那么$S$在给定的封锁协议下是合法的。如果一种封锁协议的所有合法调度都是冲突可串行化的，那么这个封锁协议就保证了冲突可串行化。 锁的授予当事务对数据项申请的锁没有被其他事务在同一个数据项申请的锁排斥的时候，锁就可以被授予。但是申请锁有可能导致饿死，可以当一个事务申请M锁的时候，必须保证Q（数据项）上冲突的锁在其他事务上是不存在的，并且没有其他事务比当前事务更先提出加锁，这样就可以避免饿死的产生。 两阶段封锁协议该协议分为两个阶段 增长阶段：一个事务可以获取锁，但是不能释放任何锁 缩减阶段：一个事务可以释放锁，但是不能获得任何新锁这个协议可以保证可串行化。但是不能保证死锁。其中事务中获取最后锁的位置叫做封锁点。 这个就是一个遵守两阶段封锁协议的事务 严格两阶段封锁协议希望事务是冲突可串行化之外，还希望事务是无级联的，在两阶段封锁协议中，级联回滚是可能发生的而严格两阶段封锁协议则要求事务持有的所有排他锁必须在事务提交后释放，这个要求保证未提交事务所写的任何数据都需要在事务提交前以排他模式封锁。 强两阶段封锁协议这个协议要求事务提交前保留所有的锁。这样就可以保证事务可以按其提交的次串行化。 锁转换当一开始使用共享锁，然后需要执行写操作时，可以使用锁升级，反之使用锁降级，这样可以提高并发度。 广泛使用的机制这个机制自动的为事务生成封锁和解锁指令。 当事务$T_i$发出read(Q)指令的时候，系统会产生lock-S(Q)指令，然后再产生read(Q) 当事务发出write(Q)操作的时候，系统会检查是否有共享锁，如果有，升级成排斥锁，然后接着生成write(Q)操作，否则先lock-X(Q)，接着再write(Q) 当一个事务提交或者回滚后，该事务所有的锁都会被释放 封锁的实现封锁是用了锁管理器来管理锁，它从事务接受消息并应答。锁管理器对封锁消息请求采用授予的信息来应答，或者采用要求事务回归的消息来应答， 对于解锁的消息只需要用于确认来回答。锁管理器使用了hash table和链表还作为数据结构，每个数据项维护一个记录的链表，每一个请求对应链表中的一条记录，并按照请求到达的顺序进行排序。其中hash table称作锁表，对于每个数据项，链表中的每条记录都记载了哪个事务，锁的模式等信息。锁表应该还要维护一个基于事务表示的索引，来确定一个给定事务的所有锁。 基于图的协议如果开发非两阶段的协议，需要每个事务如何存取数据库的附加信息。在数据项中规定了一种偏序$\\rightarrow$ 如果数据项$d_i \\rightarrow d_j$，那么任何即访问$d_i$又访问$d_j$的事务必须先访问$d_i$再访问$d_j$，这样就形成了数据库图。 书中给出了基于树形协议的简单协议，这个协议规定必须使用排他锁，并且对于每个事务 首次封锁可以在任何数据项上进行 此后，事务可以对数据项加锁的前提是该事务由该数据项父项上的锁。 数据项解锁可以随时进行 一个数据项被事务加锁并解锁后，事务不能再对该数据项加锁。树型协议保证冲突可串行化，并且保证不会产生死锁。如果要保证恢复性和无级联性，可以让事务结束前不允许释放排他锁。如果只想保证可恢复性，当$T_i$事务执行对未提交数据读操作的时候，就对该数据执行写操作的事务记录一个$T_i$的提交依赖，然后当事务$T_i$的提交依赖的所有事务完成提交之前，$T_i$不允许提交。 优缺点优点 不会产生死锁，无需回滚 可以较早的释放锁，提高并发缺点 事务可能需要给根本不访问的数据项加锁，导致并发度降低 死锁处理有两种主要的方法来处理死锁问题，可以使用死锁预防，也可以让系统允许进入死锁状态，然后试着用死锁检测和死锁恢复来进行回复。 死锁预防死锁预防有两个方向，一个是给封锁请求一定顺序，或者要求事务要同时获取所有需要的数据项的锁再开始，另一个方向是每当等待有可能导致思索地时候，就执行事务回滚而不是等待锁。第一种方法最简单的机制是事务执行前封锁它所有所需要的数据项，另一种方法是对数据项加一种次序，要求事务只能按照次序来顺序封锁数据项，树形协议就是这种方式。还有一种方式是，给所有数据项来个整体的次序，当事务封锁数据项时，不能再次封锁这个数据项之前的数据项。 第二种方式就是使用抢占和事务回滚。 死亡-等待(wait-die)机制是非抢占技术，当事务$T_i$时间戳小于（出现时间晚）$T_j$的时间戳的时候，如果$T_j$占有了$T_i$需要的数据项，那么$T_i$等待，如果$T_i$的时间戳大，那么$T_i$回滚 伤害-等待(wound-wait)，和前者类似，但是当$T_i$时间戳小于$T_j$的时候，$T_j$回滚 锁超时是另一个种简单的方式，申请锁的事务至多等待一段指定的时间，如果超时，那么事务自己回滚。 死锁检测死锁检测可以使用等待图来描述，当事务$T_i$等待$T_j$的时候，那么就建立一条边，如果有环路，那么就说明出现了死锁 死锁恢复死锁恢复解决方案主要是回滚一个或者多个事务，需要采取三个动作 选择牺牲者 回滚 饿死 如果总是选择相同的事务作为牺牲者，那么有可能会产生饿死的情况，必须保证事务被选为牺牲者的次数是有限的 多粒度多级力度封锁协议使用了树形的数据粒度的层次结构，树中的每个节点都可以单独加锁，并且会隐式的给子节点加相同的锁。 意向锁如果有事务想要封锁整个树，就需要遍历所有的节点判断是否加的锁和某个节点的锁有冲突，这样不高效。可以给节点加上意向锁，意味着会给该节点的子节点显式加锁，这样如果要给一个节点加锁时，就可以遍历该节点的上级节点们，给他们加上对应的意向锁。意向锁有IS IX SIX三种，IS意思是要读取某个子节点，也就是会给某个子节点加上共享锁，IX意思是要修改某个子节点，会给某个子节点加上排斥锁。对于SIX来说，意思是会读取整个子树，并修改某个节点，所以会给加上SIX的节点添加共享锁，并会在需要修改数据的子节点上加上排斥锁。 例子1：IS和IX的兼容例子2：SIX和IS兼容，但是和IX不兼容 冲突图","link":"/2022/05/28/CMU-15-445%E7%AC%94%E8%AE%B0-%E5%8D%81%E4%BA%94/"},{"title":"CMU-15-445笔记(十四)","text":"Transactions事务包含了一系列的operations，并且不允许部分执行，要么全部执行，要么不执行。想要实现事务一个比较好的方式是用并发来实现，这样每个事务可以独立执行。 Formal DefinitionsDatabase: 是由一系列数据组成Transaction: 一系列读写操作 Transaction in Sql事务由 BEGIN开始，然后终止于COMMIT 或者ABORT，如果是commit，那么事务的操作就保存下来了，如果是abort，那么事务的操作会全部回滚。 ACIDAtomicity原子性就是事务要么全部执行，要么全都不执行。 实现方式 Loggin DBMS记录所有的actions，以便回滚的时候再回复数据 DBMS会将undo records保存在内存和磁盘中。 loggin需要在data写入disk之前写入disk shadow paging 执行事务的时候，复制事务会修改的page，所有修改都在副本上执行，当事务执行完成的时候将page指针指向副本即可。shadow paging由于会产生磁盘碎片，而由于logging是批量的写入磁盘，所以是顺序写入，所以速度会比shadow paging快 Consistency有两种一致性，数据库一致性和事务一致性。 Database Consistency通过完整性约束来保证数据库一致性。 Transaction Consistency如果数据库在执行事务前是一致性正确的，那么执行事务之后也应当是一致性正确的。 Isolation隔离性（isolation）指的是不同事务在提交的时候，最终呈现出来的效果是串行的 实现实现是由concurrency control protocol来实现的，它指挥DBMS来决定执行不同操作的顺序。有两种协议 Pressimistic（悲观协议） 在执行事务前获取lock，来保证问题不会发生 Optimistic（乐观协议） 会假设问题很少发生，所以不会获取lock，当事务提交前会检查是否有问题发生。 例子假设现在有A，B两个账户，每个账户都有1000元，现在有两个事务 123456789BEGINA = A - 100B = B + 100COMMITBEGINA = A * 1.06B = B * 1.06COMMIT DBMS不用保证T1和T2的执行顺序，但是不论哪种顺序，最后结果A+B的和应该为2120当一个事务需要执行IO操作的时候，可以等待它的同时执行另一个事务。但是如果是另一种情况就会导致最后数据和期望数据不同 如何保证正确性Serial Schedule: 不同 transactions 之间没有重叠Equivalent Schedules: 对于任意数据库起始状态，若两个 schedules 分别执行所到达的数据库最终状态相同，则称这两个 schedules 等价Serializable Schedule: 如果一个 schedule 与 transactions 之间的某种 serial execution 的效果一致，则称该 schedule 为 serializable schedule Conflicting当两个operations满足三个条件的时候，是conflicting operations: 来自不同的 transactions 对同一个对象操作 两个 operations 至少有一个是 write 操作 主要有三种类型： Read-Write Conflicts Write-Read Conflicts Write-Write Conflicts Read-Write Conflicts会导致不可重复读的问题 Write-Read Conflicts会导致脏读 Write-Write Conflicts会覆盖掉未提交的数据 Conflict Equivalent Two schedules are said to be conflict equivalent when one can be transformed to another by swapping non-conflicting operations. Conflict Serializable Schedules A schedule is called conflict serializable if it can be transformed into a serial schedule by swapping non-conflicting operations. 客观评价，这块视频讲的不太清楚，可以去看geeksforgeesk的讲解判断是否是Conflict Serializable，可以通过交换non-conflicting operations的执行顺序后，看是否和Serial Schedule结果相同这可以说明是Conflict Serializablilty的 这个就不是 Dependency Graphs一种更方便的方式是构建依赖图，如果图没有环路，那么说明是conflict serializable的。图的边的意思是在事务Ti和事务Tj中，操作Oi（来自Ti）与Oj（来自Tj）冲突，并且Oi比Oj出现的早，那么就建立一条Ti到Tj的边 如果有环路，就说明不是conflict serializable 没有环路就是conflict serializable View SerilizabilityView Serilizability是一种更弱的Serilizability，它不保证conflict serializable，但是保证最后结果是和serializable一致的。 SerializabilityConflict Serialization是基本所有数据库都支持的，但是View Serilizability现在不常见 这是Schedules的总览图 Durability所有的事务提交后的信息都应该是持久的，DBMS可以使用logging或者shadow page来确保这一点。","link":"/2022/05/22/CMU-15-445%E7%AC%94%E8%AE%B0-%E5%8D%81%E5%9B%9B/"},{"title":"CMU15-445 Lab3记录","text":"Task 1 Excutors这次lab介绍非常非常长，要完成9个executors，有sequential scan, insert, update, delete, neted loop join, hash join, aggregation, limit, distinct，每一个种query plan都有init next方法，init是用来初始化的，next是提供给调用者一个tuple和对应的RID 每一个excutor都负责执行一个plan node type。plan node是一个query plan的单独模组。每个plan node可能定义了一个operator需要的特定的信息。Executors 接受来自子节点的tuples，然后处理后把这些tuples给parent。可能executors依赖children的顺序(?)本次实验并不需要保证是并发安全的。project提供了一个ExecutionEnginehelper class，它将input query转化为一个query excutor，然后执行这个excutor。我们需要修改ExecutionEngine来捕获异常。ExecutorFactor是负责在执行query exection时创建excutors的，每个executor都会在执行的时候访问ExcutorContext。当实现类似insert, update之类的修改类型的函数的时候，需要保证index的一致性，所以这些executors选哟去更新修改表的所有index。可以使用上次实现的extendible hash table来作为index的数据结构 Sequential Scan这个较为简单，只需要从exec_ctx_中获取table中有关tuple的迭代器，遍历这个迭代器，然后根据plan中的GetPredicate，判断当且遍历到的tuple是否符合查询的条件，如果符合，那么就将tuple转换成plan中的格式，最后赋值给*tuple即可 123456std::vector&lt;Value&gt; values; values.reserve(plan_-&gt;OutputSchema()-&gt;GetColumnCount()); for (const auto &amp;col : plan_-&gt;OutputSchema()-&gt;GetColumns()) { values.push_back( col.GetExpr()-&gt;Evaluate(&amp;(*ptr_), &amp;exec_ctx_-&gt;GetCatalog()-&gt;GetTable(plan_-&gt;GetTableOid())-&gt;schema_)); } Insert/Delete/Update这三个大同小异，通过child plan或者从plan中获取的tuple，使用tableheap中的insert/delete更改表中的tuple，最后更新index中的信息。 123456std::vector&lt;IndexInfo *&gt; index_info_array = exec_ctx_-&gt;GetCatalog()-&gt;GetTableIndexes(exec_ctx_-&gt;GetCatalog()-&gt;GetTable(plan_-&gt;TableOid())-&gt;name_); for (auto &amp;index_info : index_info_array) { index_info-&gt;index_-&gt;InsertEntry(tuple-&gt;KeyFromTuple(exec_ctx_-&gt;GetCatalog()-&gt;GetTable(plan_-&gt;TableOid())-&gt;schema_, index_info-&gt;key_schema_, index_info-&gt;index_-&gt;GetKeyAttrs()), *rid, exec_ctx_-&gt;GetTransaction()); nested loop join通过双重循环，判断left tuple和right tuple是否符合plan中的条件即可， 如果符合，通过col中的evaluatejoin来将left tuple和right tuple join起来 1234567if (join_predicate_ == nullptr || join_predicate_-&gt;EvaluateJoin(&amp;outer_tuple_, left_schema_, &amp;inner_tuple_, right_schema_).GetAs&lt;bool&gt;()) { std::vector&lt;Value&gt; values; values.reserve(GetOutputSchema()-&gt;GetColumnCount()); for (const auto &amp;col : GetOutputSchema()-&gt;GetColumns()) { values.push_back(col.GetExpr()-&gt;EvaluateJoin(&amp;outer_tuple_, left_schema_, &amp;inner_tuple_, right_schema_)); } hash join这个需要参考aggregation来写一个hash table，key为tuple经过left plan转成的对应Value，value为vecotr&lt;Tuple&gt;，因为可能多个tuple对应了一个key，之后遍历right tuple，去看table对应的tuple（可能有多个），最后调用EvaluateJoin即可。 aggregation这个核心代码都实现好了，只需要在init的时候构建对应的table即可。 123while (child_-&gt;Next(&amp;tuple, &amp;rid)) { aht_.InsertCombine(MakeAggregateKey(&amp;tuple), MakeAggregateValue(&amp;tuple)); } 然后在Next，如果没有having子语句或者符合having，那么返回迭代器当前指向的值就可以了。 123456789101112while ( aht_iterator_ != aht_.End() &amp;&amp; !have_node-&gt;EvaluateAggregate(aht_iterator_.Key().group_bys_, aht_iterator_.Val().aggregates_).GetAs&lt;bool&gt;()) { ++aht_iterator_; } if (aht_iterator_ == aht_.End()) { return false; } for (auto &amp;col : plan_-&gt;OutputSchema()-&gt;GetColumns()) { values.push_back( col.GetExpr()-&gt;EvaluateAggregate(aht_iterator_.Key().group_bys_, aht_iterator_.Val().aggregates_)); } distinct和limit这两个很简单，就不记录了","link":"/2022/05/15/CMU15-445-Lab3%E8%AE%B0%E5%BD%95/"},{"title":"CMU15-445笔记(七)","text":"Hash Table Latching","link":"/2022/04/30/CMU15-445%E7%AC%94%E8%AE%B0-%E4%B8%83/"},{"title":"CMU15-445笔记(九)","text":"JoinOperator OutputDataEarly Materialization将需要join的数据复制到新的tuple中，这样做的好处就是接下来的子操作不会再回到原来的tuple进行更多的操作。 Record IdsLater Materialization只复制join匹配上的主键，以及join key，当需要数据的时候，直接去相应的table中获取 I/O Cost Analysis Criteria衡量join algorithm好坏的标准在于I/O的数量接下来的讨论会基于以下假设：表R，有M个pages和m个tuples表S，有N个pages和n个tuples Join AlgorithmNested Loop Join就是for循环，这种方法是最暴力的方式Cost是M + (m * N) Block Nested Loop Join将outer table用block来读取（也可以是一个page），inner table用另外一个block读取，这样可以减少io次数如果有多个buffer page 的话，假设为B个，其中一个用于inner table，另一个用于output， 那么outer table就可以一次缓存 B-2个page这样cost也会减少 Index Nested Loop Join给inner table使用index（可以是B+树），来加快查找的速度 Sort Merge Join有两个步骤 Sort 将inner和outer table通过join key进行排序 这里排序可以使用上一个笔记当中说到的各种排序方式 Merge 通过扫描（双指针）两个表的key，来达到join的目的 Hash Join大概思路就是先将outer table的key通过hash进行分区，然后将inner table的key也通过相同的hash分区到和outer hash值一样的区域，然后根据这些分区来逐个进行join，主要步骤有两个： Build 扫描outer table 然后用假设叫h1的hash function 来hash join属性到hash table中 Probe(探测) 扫描inner table，用h1来hash join属性，然后取定位到outer table的hash tabel中，这样就能找到匹配到的tuple 这里hash table的key就是需要join的字段，value取决于具体实现 Hash Table Values有两个方法 Full Tuple 将整个tuple放入hash table中 这样可以避免来回的查找，因为hash table中已经有全部的需要的字段，但是也会占用大量的空间 Tuple Identifier 直接用tuple record id，需要数据的时候去disk里面取 减少大小 Probe Phase Optimization可以使用布隆过滤器，来判断key是否存在于hash table中（可能会误判不存在为存在，但是无关紧要），由于布隆过滤器很小，所以可以直接放入内存中，这样就可以让inner table查找outer key是否存在的时候，先看布隆过滤器，然后再去找hash table(因为hash table 会很大，所以内存放不下，读取需要多次IO)，这样就减少了IO次数 Bloom Filters使用了bitmap来回答是否数据存在，有误差，可能将不存在判断为存在 Grace Hash Join与常规的Hash Join 不同，Grace Hash Join两个table都需要进行hash，然后通过遍历相同hash值的bucket来进行join 如果bukckets太大了，放不进内存中就有 Recursive Partitioning(递归分区)如果其中一个bucket太大了， 就可以使用另一个hash将这个bucket继续拆分 hash join的cost Summary 结论hash总是一个比sort更好的方法，例外是 当数据分布为及其不均匀的 当结果需要sort","link":"/2022/05/14/CMU15-445%E7%AC%94%E8%AE%B0-%E4%B9%9D/"},{"title":"CMU15-445笔记(八)","text":"Sort2-Way-External Merge Sort因为数据库的需要排序的数据可能内存装不下，所以可以分块的来进行排序。 首先每次读取B个pages进入内存中 然后将他们排序后的副本放入disk中 取两个已经排好序的副本进行归并排序，循环，直到所有都排好序其实就是一个归并排序每次从disk中读取页再写回的时候，用到了两次IO，所以每次传递的IO总共是2 * N I/O，需要$log_2(N)$的排序次数，最终加上第一次传递的IO，最终总的IO是$$ 2N * (1 + log_2(N))$$可以从图中看出，每次排序的时候，需要两个buffer page 来存储即将要排序的两个页，然后再有一个page 来存储排序后的结果，所以一共需要三个 buffer page Double Buffering Optimization就是当一个run 正在排序的时候，将另一个即将要排序的run 预读取到另一个buffer 中 General External Merge Sort Using B+ Trees For SortingClustered B+Tree聚簇索引中的B+树的叶子叶子节点已经排好序了，所以直接遍历就行 Unclustered B+Tree非聚簇索引中，叶子节点存储的是数据的主键/指针，所以需要多进行一次IO才能获取数据 Aggregations实现有两种方式sort 和 hash Sort Hash有一些是聚合是不需要排序（Group By Distinct）的，普遍情况下Hash速度更快对于Distin来说，只需要丢弃相同hash值的数据就可以了，Group By只需要将相同hash值的数据聚合在一起。对于数据过多，内存容量不够的情况来说需要External Hashing Aggregate External Hashing Aggregate当内存不足的时候，假设现在有B个buffer page，我们可以使用B-1个buffer page当作分区（少的那一个作为input data的buffer page），对于每个key，我们用h1来进行hash操作，将每个key的数据分别放到B-1个buffer page当中，称作分区，然后分别取出每个分区，对每个分区使用h2来进行hash，将key放入hash table（需要处理最后的碰撞）中，这样就可以得出最后结果。","link":"/2022/05/11/CMU15-445%E7%AC%94%E8%AE%B0-%E5%85%AB/"},{"title":"CMU15-445笔记(十六)","text":"并发控制基于时间戳的协议时间戳对于系统中的每个事务$T_i$，赋予它一个时间戳，记为$TS(T_i)$。如果$TS(T_i) &lt; TS(T_j)$，说明$T_j$比较新。可以使用 系统时钟 逻辑计数器 来实现时间戳。除了$TS(T_i)$之外，还需要两个与数据库项$Q$相关的时间戳 W-timestamp(Q)表示成功执行write(Q)的任意事务的最大时间戳 R-timestamp(Q)表示成功执行read(Q)的人一十五的最大时间戳 时间戳排序协议时间戳排序协议保证任何有冲突的read和write操作按照时间戳的次序执行。 假设事务$T_i$执行read(Q) 如果$TS(T_i) &lt; W-timestamp(Q)$，那么$T_i$需要读取的值就被覆盖了，read操作会被拒绝执行，并且执行回滚 如果$TS(T_i) \\geq W-timestamp(Q)$那么可以执行read操作，并且$R-timestamp(Q)$会被设置成$R-timestamp(Q)$和$TS(T_i)$的最大值 如果事务$T_i$执行write(Q) 如果$TS(T_i) &lt; R-timestamp(Q)$ 那么说明$T_i$的Q值是之前需要的，并且系统会认为这个值不会再出现了，所以拒绝执行，回滚 如果$TS(T_i) \\geq R-timestamp(Q)$，那么执行，并将$W-timestamp(Q)$设置为$TS(T_i)$ 时间戳排序协议保证没有死锁，因为事务不会出现等待锁的情况。但是有可能会出现饿死的情况。时间戳协议也可能会出现不可恢复的调度。可以采取以下策略来保证可恢复 再事务末尾一起执行所有的写操作，并且要求再执行这些写操作的时候，任何事务都不能访问已经写过的任何数据项，否则就有可能不可恢复。这种策略是保证无级联和可恢复的 对未提交的数据项的读操作推迟到更新该数据项的事务提交之后也可以保证无级联和可恢复 使用提交依赖 Thomas写规则当$TS(T_i) &lt; W-timestamp(Q)$的时候，这个write会直接忽略，而不是回滚。这个会形成view serializable，不是冲突 serializable。 基于有效性检查的协议有效性检查时用来监控哪些事务会陷入冲突中的监控机制。它要求事务$T_i$按照两个或者三个阶段来执行。这个协议也叫乐观的并发控制(optimistic concurrency-control) 读阶段 在这个阶段中，事务$T_i$读取数据项的值并保存在局部变量中，$T_i$的所有write操作都是针对局部变量的 有效性检查阶段 对事务$T_i$进行有效性检查测试，决定事务是否可以继续执行，还是终止 写阶段 如果通过了检查，$T_i$将write的操作的局部变量写入数据库 有效性检查为了执行有效性检查，需要以下三个时间戳 $StartTS(T_i)$ 事务开始执行的时间 $ValidationTS(T_i)$ 事务完成读阶段并开始检查的时间 $FinishTS(T_I)$ 事务完成写阶段的时间 该协议利用$ValidationTS(T_i)$时间戳的值来通过时间戳排序协议决定串行化的顺序。所以$TS(T_i)=ValidationTS(T_i)$，并且如果$TS(T_j) &lt; TS(T_k)$，那么有效性检查协议必须等价于事务$T_j$出现在$T_k$之前的一个串行调度。 有效性检查需要满足以下两个条件之一 $FinshTS(T_k) &lt; StartTS(T_i)$，这样的话，$T_k$不会影响到$T_i$ $T_k$写的数据项和$T_i$读的数据项没有交集，并且在$T_i$开始有效性检查阶段前，$T_k$就完成了写阶段，这就保证了两个事务的写不会重叠，导致覆盖的顺序错误。这样就保证了串行化 有效性检查还保证了无级联，因为写操作事务提交后实际上的写到数据库的操作才开始。 为什么要用$ValidationTS(T_i)$来作为$TS(T_i)$，而不是$StartTS(T_i)$，还是假设$TS(T_i) &gt; TS(T_k)$这样有可能导致$T_i$比$T_k$更早的进入有效性检查阶段，从而导致$T_i$需要等待$T_k$ 多版本控制multiversion concurrency control在每次write(Q)时都会创建Q的新版本，当事务执行read(Q)操作的时候，并发控制管理器会选择Q的一个合适的版本进行读取，必须能保证可串行化。多版本控制主要解决的时以上协议想要读取旧版本时导致的回滚或者等待问题。 多版本时间戳排序这个是时间戳排序协议的拓展版本，对于每一个数据项Q，都有多个版本Q1 Q2…Qk，每个版本都对应着三个数据字段 Content Qk版本的值 W-timestamp(Q) 创建Qk版本的事务的时间戳 R-timestamp(Q) 所有成功读取Qk事务的最大时间戳 假设$Q_k$小于或等于$TS(T_i)$的最大写时间戳 如果$T_i$执行read(Q)，那么直接返回$Q_k$的值 如果事务$T_i$执行write(Q)，如果$TS(T_i) &lt; R-timestamp(Q_k)$，那么系统回滚$T_i$，如果$TS(T_i)=W-timestamp(Q_k)$，系统会覆盖Q_k的内容，如果$TS(T_i) &gt; R-timestamp(Q_k)$，系统给Q创建新的版本 对于Q，有一个有效区间的概念，如果Qi是Q的最新版本，那么Qi的有效区间就是$[W-timestapm(Q_i), \\infty]$，如果不是最新的，那么就是$[W-timestamp(Q_i), W-timestamp(Q_{i+1})]$。有效区间就是事务在这个区间内会读取到这个Q版本的数据 多版本时间戳是不保证可恢复和无级联的，可以按照原版时间戳的方式进行扩展。 多版本两阶段封锁多版本两阶段封锁会对只读事务和更新事务分别有不同的处理更新事务会执行强两阶段封锁，事务会只有全部的锁直到事务结束，所以可以实现按照事务提交的次序来进行串行化，每个事务都会有以逻辑计数器呈现的时间戳，叫ts-counter当只读事务开始前，数据库会赋予它时间戳，只读事务执行读操作是和多版本时间戳是一样的。当更新事务读取一个数据项的时候，会先获取数据项上的共享锁并且读取数据项的最新版本。当更新事务写数据的时候，要获取排他锁，然后创建一个该数据项的新版本，并设置新版本的时间戳为$\\infty$当更新事务$T_i$完成操作的时候，会进入提交过程。同一时间只能有一个更新事务进行提交。$T_i$会将它创建的每个版本的时间戳设置为ts-counter + 1, 然后ts-counter + 1，最后提交。只读事务会看到ts-counter的旧值直到$T_i$提交陈工，所以$T_i$提交前，启动的只读事务只能看到$T_i$更新前的就值，而提交后的只读事务能看到新值。只读事务在任何情况下都不用加锁。 多版本两阶段封锁是可恢复且无级联的（因为强两阶段封锁） 多版本控制删除版本如果有多个W-timnestamp小于当前所有事务中最老的事务，那么就删除这些版本除了最新的版本之外的所有版本。 快照隔离快照隔离是一种特殊的并发控制机制。在事务开始执行时给它一份数据库的快照，然后事务在快照上以其他并发事务完全隔离的形式进行操作。快照中的数据值仅包含已经提交的事务所写的值。在快照隔离中，更新事务会存在可能的冲突，所以在允许事务提交前，需要进行有效性检查，这里的有效性检查和前面的方式不同。直到检查通过，事务进行的更改才会进入数据库中。 快照隔离中的多版本事务有两个时间戳，$StartTS(T_i)$和$CommitTS(T_i)$，分别是事务$T_i$开始的时间和事务请求有效性检查的时间。在版本中，只有一个写时间戳，表明版本创建的时间，事务$T_i$创建的版本的时间戳设定为$CommitTS(T_i)$当事务$T_i$读取一个数据项的时候，具有时间戳$\\leq$ 的数据项的最新版本会返回给事务。所以事务$T_i$不会看到比事务新的数据，这样就完成了快照隔离 更新事务的有效性检查步骤如果有两个更新事务都要进行提交，有可能会出现更新丢失的情况，快照隔离有两种方式来防止更新丢失 先提交者胜 first committer wins 先更新者胜 first updater wins这两种都是基于检查事物的并发事务 并发事务如果：$StratTS(T_j) \\leq StartTS(T_i) \\leq CommitTS(T_j)$$StratTS(T_i) \\leq StartTS(T_j) \\leq CommitTS(T_i)$两个任意一个成立，那么这两个事务就是并发的 先提交者胜 first committer wins 检查是否存在某个事务在当前事务将数据写到数据库前就将更新提交到了数据库中，也就是可以检查数据项d中是否有一个版本的时间戳介于$StratTS(T_i)$ $CommitTS(T_i)$之间 如果有，那么$T_i$就终止 如果没有就提交 先更新者胜 first updater wins当事务想要更新数据项的时候，先获取该数据项的写锁，如果能获取，那么获取后 如果这个数据项已经被任何并发事务更新，那么$T_i$终止，否则可以继续执行如果写锁已经被另一个并发事务获取了， $T_i$等待直到获取锁的事务中止或者提交 如果获取锁的事务终止了，那么$T_i$获取锁，然后执行上面的有效性检查 如果获取锁额事务提交了，那么$T_i$中止 快照隔离存在的问题快照隔离第一个问题是主键约束和外键约束不能再快照","link":"/2022/05/28/CMU15-445%E7%AC%94%E8%AE%B0-%E5%8D%81%E5%85%AD/"},{"title":"Extendible Hashing","text":"参考 https://www.geeksforgeeks.org/extendible-hashing-dynamic-approach-to-dbms/ 简介 术语Extendible Hashing 是一个动态的hash方法，有directorites和buckets，用于hash data。 Directiones: 主要用来存放buckets的指针，当direcory扩展的时候，索引direction的id会改变 Buckets: 用于存储实际的数据 Global Depth: 表示当前的hash值有多少位被用于索引Directories。 Local Depth: 这个和bukets相关联，比如01,00都指向同一个buket，那么这个buket的local depth 就是1，因为只有一位是索引它的。 Bucket Splitting: 当一个bucket的元素超过了page大小，bucket就会split两个部分 Director Expansion: 当一个bucket overflows的时候，且local depth 等于 global depth的时候，就会发生 Extendible Hashing的基本步骤 将数据转化为二进制形式 确定当前的global depth 以LSB的形式来看二进制形式后的数据对应的directory，例如global depth为3的情况，11001，对应001directory 跳转到bucket 执行插入操作并检查bucket是否overflow了 如果overflow了 情况1 如果overflow Bucket的local depth 等于当前的global depth，那么执行Directory Expansion，同时Bucket Split 也需要执行。最后增加一位global depth，并增加指针 情况2 如果local depth小于当前的global depth，那么只执行bucket split，增加local depth。 rehash split bucket样例当插入22的时候，产生overflow，那么此时就扩展directory，以及重新hash，更新global depth和local depth当再次插入满的时候，继续判断是否local depth 等于 global depth，如果等于，那么还是执行上述操作如果小于global depth，那么直接split bucket就可以了","link":"/2022/04/18/Extendible-Hashing/"},{"title":"MapReduce阅读笔记","text":"来自论文 MapReduce: Simplified Data Processing on Large Clusters 介绍MapReduce使用了map函数来将key/value pair生成中间的key/value pair，然后用reduce函数将所有中间结果的value通过相同的key聚合起来 Programming Model模型主要有两个函数，map和reduce MapMap函数是由用户来写的，将输入的数据处理为一系列中间结果的key/value pairs。MapReduce将所有相同key的value聚合在一起，传递给Reduce函数 ReduceReduce函数也是由用户来编写的，接受key $I$和对应的values，它将这些values合并成尽可能小的value的集合。 例子1234567891011121314map(String key, String value): // key: document name // value: document contents for each word w in value: EmitIntermediate(w, &quot;1&quot;);reduce(String key, Iterator values): // key: a word // values: a list of counts int result = 0; for each v in values: result += ParseInt(v); Emit(AsString(result)); 这个例子是计算文档中每个单词出现的个数map函数将每个单词，和1发送出去reduce函数将map发送的数据中相同单词的1全部加起来，就是最后该单词在文档中出现的数量 Types用户提供的map和reduce函数有相关联的类型 12map(k1, v1) -&gt;list(k2, v2)reduce(k2, list(v2)) -&gt;list(v2) 输入的keys，values和输出的keys和values来自不同的域。但是中间结果的keys, values和输出的keys，values来自同一个域(这一句话不是太能理解…) 实现 首先MapReduce将用户输入的文件分割$M$个份(通常为16MB到64MB)，然后在集群中启动多个程序 其中一个程序为Master，剩下的为wokers，听从master的调度来辅助工作。其中有$M$个map任务和$R$个reduce任务。master会选取空闲的wokers分配给它map/reduce任务 一个被分配到map的worker会读取被分割的输入，解析出key/value pairs，然后将这些pairs输入到Map function(用户定义)，Map function会输出中间结果的key/value pairs并存放到内存中 每个放到内存中的pairs会被周期性的写入到磁盘中，并通过partitioning function分区成$R$个区域。这些pairs在硬盘存放的位置会告知master，然后master会将位置再传递给reduce函数 当一个reduece worker被master通知中间pairs存放在磁盘的位置时，会读取一个区域中的数据，当所有的中间结果的数据读取到后，它会将这些数据以key来排成有序，这时候，有相同key的value就会合在一起 reduce woker会遍历所有已排好序的中间结果，然后对于每一个key，以及对应的values的set，都会送到用户定义的Reduce函数中，Reduce函数将最后的结果放到这个reduce区域的最后输出的文件中。 当所有的map任务和reduce任务都结束的时候，master被唤醒，将最后结果返回给用户 MapReduce全部完成的时候，会产生$R$个输出文件，每个对应一个reduce产生一个，通常用户不用去将它们合并，因为它们大部分时间会被传送到下一个MapReduce中，或者下一个分布式的处理函数中。 Master Data Structuresmaster需要保存多个数据结构，对于每个map task和reduce task，master需要保存它们的状态，以及标识信息master还需要保存R个中间key/value pairs存储的位置，以及大小。并且在每个map task完成后更新它。 容错Woker Failuremaster会周期性的ping每个woker，如果有woker在指定的时间内未回复，那么master就会标记这个woker失败了。当任意一个map task完成后，其对应的woker就会设置为idle状态，因此它可以被安排到其他任务上。同样当有一个map或者reduce失败的时候，其woker也会设置为idle等待重新调度。当map task失败的时候，需要重新执行该task，因为map task将临时的数据存储在了本地磁盘中，无法在全局中获取。reduce task失败的时候不需要重新执行，因为它们的数据存放在了全局的文件系统。当一个map task先由woker A执行，当A失败后，再由B执行，此时会通知所有的reduce task，告知它们以后读数据从B中读取。 Masker Failuer可以给master设立检查点，当master是失败的时候，可以让新的master从上一个检查点开始。 Semantics in the Presence of Failures当用户提供的map和reduce操作输入值确定时，输出值也确定，那么分布式实现与不出错的顺序整体实现输出应当是一致的。MapReduce通过map和reduce task使用atomic commites来实现上述特性。每个运行中的task都会将它们的输出写到私有的临时文件中。一个reduce task会产生一个这样的文件，一个map会产生R个这样的文件（每个文件对应一个reduce）。当一个map task完成后，对应的woker会想master发送完成信号以及R个临时文件的信息。当master重复接收到一个已经完成的map task的信息后，就会忽略这个重复的信息。如果不重复，那么master就会记录这R个文件的信息。当一个reduce task完成后，reduce woker就会原子化的重命名它临时输出文件从而作为最后的输出文件。//TODO: 较弱的失效处理没看懂… Locality论文中的输入文件是由GFS管理的，GFS将大文件分割成64MB的块，然后复制几份（一般是三份）放到不同的机器中，MapReduce master就会调度这些有复制文件的机器执行map task。如果调度失败，那么master就会调度里数据比较近的机器执行map Task Granularity// TODO","link":"/2022/04/27/MapReduce%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"title":"redis设计与实现笔记(一)","text":"字符串Redis的字符串用的是自己构建的字符串，称作SDS(简单动态字符串)，除了用于保存数据库中的字符串以外，SDS还被用作缓冲区。 定义 free代表是否分配空间 len代表长度 buf是char数组 获取Redis的SDS的长度为O(1)复杂度 长度当SDS长度即将溢出的时候，会自动扩容。类似vector，SDS会进行空间额预分配，当SDSDE长度小于1MB的时候，预分配1倍的空间，大于1MB的时候，预分配会多分配1MB。 惰性空间释放缩短SDS的时候，程序不会回收字节，而是仅做一下标记，增长free字段的长度。 二进制安全SDS所有的API都会以处理二进制的方式来处理存放在buf数组里的数据，不会做限制。 和C的主要区别 字典Redis中的字典是哈希表，","link":"/2022/06/14/redis%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E7%AC%94%E8%AE%B0-%E4%B8%80/"}],"tags":[{"name":"bug","slug":"bug","link":"/tags/bug/"},{"name":"OS","slug":"OS","link":"/tags/OS/"},{"name":"CMU-15-445","slug":"CMU-15-445","link":"/tags/CMU-15-445/"},{"name":"数据库","slug":"数据库","link":"/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MIT6.824","slug":"MIT6-824","link":"/tags/MIT6-824/"},{"name":"redis","slug":"redis","link":"/tags/redis/"}],"categories":[]}
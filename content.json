{"pages":[{"title":"关于","text":"关于我一名普通的学生。","link":"/about/index.html"}],"posts":[{"title":"CSAPP第七章Linking","text":"Object FilesObject file 有三种形式 Relocatable object file Executable object file Shared object file Relocatable Object FilesCSAPP 书中选取的是ELF格式。ELF从16字节的序列作为起始，这16字节规定了字的大小和机器的字节顺序(大端或者小端)，ELH header剩下的内容帮助连接器分析解释这个object file。而各种sections的信息，如位置和大小，由section header table 给出，它包含了每个sections固定大小的入口。 sections的类型 .te xt编译后的机器码 .rodata只读数据，例如printf中的format strings 或者switch的jump tables .data初始化的全局和静态变量 .bss未初始化的全局和静态变量，并且这个section并不占据object file的实际磁盘空间，当程序运行时，这些变量会申请内存并初始化为0 .symtabsymbol table，包含了函数和全局变量的信息。 .rel.rext是.text section中需要重定位的位置的列表，例如外部函数或者外部的局部变量。和并不包含没有用到的指令。 .rel.data包含这个object module中定义或者引用的全局变量的重定位信息。 .debugdebugging symbol，只有用了-g选项编译才会产生 .line .strtab","link":"/2021/09/16/CSAPP%E7%AC%AC%E4%B8%83%E7%AB%A0Linking/"},{"title":"《STL源码剖析》第二章问题杂烩","text":"近期开始阅读侯捷《STL源码剖析》，第二章有许许多多我遇到的问题，在此汇总一下 Placement new书中的使用 语法new的语法中有placement_params可选参数，当传递给new这个参数时，就是placement_new 作用placement_new就是直接将一块内存空间指定给new，new直接在这些内存上执行构造 样例1234567891011121314//样例1void* operator new(std::size_t, void*)//样例2// within any block scope...{ alignas(T) unsigned char buf[sizeof(T)]; // Statically allocate the storage with automatic storage duration // which is large enough for any object of type `T`. T* tptr = new(buf) T; // Construct a `T` object, placing it directly into your // pre-allocated storage at memory address `buf`. tptr-&gt;~T(); // You must **manually** call the object's destructor // if its side effects is depended by the program.} // Leaving this block scope automatically deallocates `buf`. false_type和true_type书中的使用书中是用来方便__destory_aux的调用，个人理解为重载，可以由__destroy直接调用两个不同作用的函数，通过false_type和true_type这两个不同的变量类型。 未完待续….","link":"/2021/08/18/STL%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90%E7%AC%AC%E4%BA%8C%E7%AB%A0%E9%97%AE%E9%A2%98%E6%9D%82%E7%83%A9/"},{"title":"Streaming Graph Neural Networks笔记","text":"Streaming Graph Neural Networks笔记 该论文提出了DGNN模型 The update componentinteract unit$$ e(t)=act(W_1 \\cdot u_{v_s}(t-)+W_2 \\cdot u_{v_g}(t-)+b_e)$$ 其中$u_{v_s}和$u_{v_g}是点在时间t之前的特征，$W_1$和$W2$以及$b_e$是神经网络的参数,$act(\\cdot)$是激活函数。$e(t)$包含了${v_s, v_g, t}$的交互的信息。 update unitupdata unit将经过intercact unit的节点信息更新。当许多interactions作用于同一节点时，update unit将遗忘以前的interactions。这操作和LSTM很像，所以论文魔改了LSTM其中target node和source node是用的两个不同的update unit，结构相同，参数不同。update unit和LSTM不同的地方在蓝色虚线中，相当于改变了LSTM中$C$的输入。公式：$$C^I_v(t-1) = tanh(W_d \\cdot C_v(t-1)+b_d)$$ $$\\hat{C}^I_v(t-1)=C^I_v(t-1)*g(\\Delta t)$$ $$C^T_v(t-1)=C_v(t-1)-C^I_v(t-1)$$ $$C^*_v(t-1)=C^T_v(t-1)+\\hat{C}^I_v(t-1)$$其中$C^I_v$由神经网络生成，代表短期记忆，短期记忆会随着时间遗忘，所以短期记忆会乘上$g(\\Delta t)$ 得出$\\hat{C}^I_v$，来表示遗忘后的短期记忆，$g(\\Delta t)$是遗忘函数，$\\Delta t$越长，其值越小。$C^T_v$是长期记忆，值不随时间而改变。$C^*_v(t-1)$是调整之后的$C$，作为标准LSTM的输入。后面就是LSTM的内容了。$$ f_t=\\delta(W_f \\cdot e(t) + U_f \\cdot h_v(t-1)+b_f) $$$$i_t=\\delta (W_i \\cdot e(t) + U_i \\cdot h_v(t-1)+b_i)$$$$o_t = \\delta (W_o \\cdot e(t) + U_o \\cdot h_v(t-1)+b_o)$$$$\\tilde{C}(t)=tanh(W_c \\cdot e(t) +U_c \\cdot h_v(t-1)+b_c)$$$$C_v(t)=f_t * C^*_v(t-1)+i_t *\\tilde{C}_v(t)$$$$ h_v(t) = o_t * tanh(C_v(t))$$ 值得注意的是，target和source node是用了两个不同的update unit。source update 只更新source node，同理target update The merge unit该unit是将当前生成的信息与之前的信息融合，形成$u_v(t)$source node 公式：$$u_{v_s}(t) = W^S \\cdot h^S_{v_s}+W^g \\cdot h^g_{v_s}(t-)+b_u$$同理target node:$$u_{v_g}(t)=W^S*h^S_{v_g}(t-)+W^g \\cdot h^g_{v_g}(t) +b_u$$ 整个update component 的流程： The propagation componentupdate component 只考虑连接建立起来时，两点的相互作用，而propagation component 则是将这两点的交互信息带入邻居节点中。影响邻居节点的方式不是像update component 那样修改cell memory的历史，而是给cell memory增添新的信息。 符号表示$N^S(\\cdot)$表示该点的source邻居的集合，$N^g(v_g)$表示该点target邻居的集合。$$N(v_s)=N^s(v_s) \\cup N^g(v_s)$$$$N(v_g)=N^s(v_g) \\cup N^g(v_g)$$ prop unit四种类型 source node $v_s$到它的source邻居$N^s(v_s)$ source ndoe $v_s$到它的target邻居$N^g(v_s)$ target node $v_g$到它的source邻居$N^s(v_g)$ target node $v_g$到它的target邻居$N^g(v_g)$ 这四种类型的prop unit结构相同，参数不同。在论文中只提供了第一种的描述，其他类型类比即可。 propagate interaction information forrmulations$$C^s_{v_x}(t)=C^s_{v_x}(t-)+f_a(u_{v_x}(t-), u_{v_s}(t-))\\cdot g(\\Delta ^s_t) \\cdot h(\\Delta ^s_t) \\cdot \\hat{W}^s_s \\cdot e(t)$$$$h^s_{v_x}(t)=tanh(C^s_{v_x}(t))$$其中$v_x\\in N^s(v_s)$，而$\\Delta^s_t=t-tx$代表了当前时间$t$与节点$v_x$与节点$v_s$发生交互的时间$t_x$的时间间隔。$f_a$是和update component中定义$f_a$是同一个衰减函数。$W^s_s$是传给source邻居交互信息的线性变换。 函数h的定义$$h(\\Delta ^s_t)=\\begin{cases}1, \\Delta^s_t\\le \\tau,\\0, otherwise.\\end{cases}$$$\\tau$是一个超参，如果时间间隔大于这个值，那么就代表时间间隔太大了。我们就不会向这个点传递交互信息。 $f_a$的定义$f_a$是捕获$v_s$和$v_x$之间连接强度的注意力函数$$f_a(u_{v_x}(t-),u_{v_s}(t-))=\\dfrac{exp(u_{v_x}(t-)^Tu_{v_s}(t-))}{\\sum_{v\\in N^s(v_s)} exp(u_v(t-)^Tu_{v_s}(t-))}$$ propagation流程图 参数学习Parameter learning for link predictionlink prediction 就是预测下一个连接关系。 projection matrixprojection matrix $P$是关于因变量(?英文名为response values)和预测值的映射。$$\\hat{y} = Py$$ $u^s_{v_s}(t-)$和$u^g_{v_g}(t-)$论文中将$u_{v_s}(t-)和$u_{v_g}$通过$P$投射到$u^s_{v_s}(t-)$和$u^g_{v_g}(t-)$ $$u^s_{v_s}(t-)=P^s \\cdot u_{v_s}(t-)$$$$u^g_{v_g}(t-)=P^g \\cdot u_{v_g}(t-)$$ loss function对于$v_s$和$v_g$出现的概率，用$$\\sigma (u^s_{v_s}(t-)^T u^g_{v_g}(t-)) $$$\\sigma(\\cdot)$代表sigmod函数。loss function: $$ J((v_s, v_g, t))=-log(\\sigma (u^s_{v_s}(t-)^T u^g_{v_g}(t-)))-Q \\cdot \\mathbb{E}{v_n\\sim P_n(v)}log(\\sigma (u^s{v_s}(t-)^T u^g_{v_g}(t-)) )$$ $Q$是负采样的数量，$P_n{v}$是一个负采样的分布。 total loss: $$ \\sum_{e\\in \\mathcal{E}(T)}J(e) $$ 其中$\\mathcal{E}(T)$代表时间$T$之前的所有交互。 训练方法采用mini-batch进行优化，并且mini-batch中边的样本是通过交互的时间序列采样的。mini-batch的loss通过所有在其中的交互计算的。负采样的分布$P_n(v)$是一个在mini-batch外所有点的标准分布，包括了交互的点和受影响的点。","link":"/2021/09/18/Streaming-Graph-Neural-Networks%E7%AC%94%E8%AE%B0/"},{"title":"《STL源码剖析》中void (* set_malloc_handler(void (*f)()))()分析","text":"问题在阅读《STL源码剖析》的时候，遇到了这么一个函数 先不说函数的意思，函数的形式我就没看懂…好多好多括号… 函数指针函数指针就是指向函数的指针… 1234567891011121314151617181920212223242526272829/* 例一：函式指標直接呼叫 复制自WIKI*/# ifndef __cplusplus # include &lt;stdio.h&gt;# else # include &lt;cstdio&gt;# endifint max(int x, int y){ return x &gt; y ? x : y;}int main(void){ /* p 是函数指针*/ int (* p)(int, int) = &amp; max; // &amp;可以省略 int a, b, c, d; printf(&quot;please input 3 numbers:&quot;); scanf(&quot;%d %d %d&quot;, &amp; a, &amp; b, &amp; c); /* 與直接呼叫函式等價，d = max(max(a, b), c) */ d = p(p(a, b), c); printf(&quot;the maxumum number is: %d\\n&quot;, d); return 0;} 在《STL源码剖析》里的这个函数，实际上使用了函数指针作为参数 1void (*f)() 函数指针作为返回值1int (*test(int))(int, int) 阅读方法是由内向外读，首先test有形参列表，所以是一个函数，并且参数只需要一个int，然后test的前面有一个*，所以返回一个指针，(*test(int))作为一个指针，有形参列表(int, int)，所以这个指针指向函数，并且这个函数返回int类型的值。 分析1void (* set_malloc_handler(void (*f)()))() 首先，set_malloc_handler作为函数名，有一个形参，是void(*f)()，void(f)()是一个函数指针类型，指向返回值为void，参数为空的函数。这说明set_malloc_handler是函数，并且前面有，所以要返回指针，指针后面接着形参列表，为空()，说明是指向函数的指针，并且指向的函数返回类型为void。","link":"/2021/08/20/%E5%87%BD%E6%95%B0%E6%8C%87%E9%92%88/"},{"title":"动态图神经网络综述笔记(一)分类","text":"本文是动态网络综述：Foundations and modelling of dynamicnetworks using Dynamic Graph NeuralNetworks: A survey 的笔记 分类根据时间粒度分类 Static没有时间信息 Edge-weighted时间信息被当做标签存放在静态网络的边/点中，最直观的例子是静态网络中边最后一次活跃时间的标签。 Discretue以离散的时间间隔表示时间信息，可以用不同时间间隔的快照来表示。 Continuous没有时间间隔，这种表示方式承载着最多的信息，但同时也最复杂。 后两者主要用来建立动态网络。 表示方法 Discrete RepresentationT代表快照的序号 Continuous Representation The event-basedrepresentationui和vi是一对连接的点，ti是时间戳，代表连接开始的时间，△i是事件(连接)持续的时间 The contact sequence representation是上一种的简化，在这种连接中，连接是instantaneous(瞬时)的，所以没有连接的持续时间。 The graph stream representation其中 ui,vi是一对连接的点，ti是事件发生的时间，最后的符号如果为1，代表边的的加入，-1代表边的删除。 根据连接时间分类 Interaction Temporal Evolving Strictly evolving 从上到下连接时间为0-无穷 根据点的动态性区分 Static点的数量始终不变 Dynamic点可能消失或出现 Growin是一种特殊的Dynamic，点只能增长 动态网络CUBEtemporal可以在没有连接的情况下存在，但对于evolving等许多网络来说不可以，当连接不存在时，点也就不存在了。三种分类可以形成一个3D的分类图 动态网络模型","link":"/2021/09/13/%E5%8A%A8%E6%80%81%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%BC%E8%BF%B0%E7%AC%94%E8%AE%B0(%E4%B8%80)/"},{"title":"动态图神经网络综述笔记(二)动态图神经网络","text":"笔记第二部分 Dynamic graph neural networks动态图网络有深度的时间序列编码，能将邻居节点聚合起来。 定义如果一个深度表征学习将邻居节点信息聚集起来作为结构，那么这就是一个动态图神经网络在离散的情况下，DGNN是GNN和时间序列的组合。如果是连续的，情况就会有些多变，聚集节点不能再用传统的GNN的方法。 DGNN的种类 Pseudo-dynamic这个方法改变的是网络的拓扑结构，而不是时间。 Discrete编码网络使用了快照，并且每次快照编码一次。 Continuous遍历网络是以边和边的形式，所以它完全独立于任何的快照尺寸。Pserudo-dynamic models pseudo-dynamic contains dynamic processes, but thedynamic properties of the model are not fit to the dynamicdata. 按照我的理解，这种方式只适用于数据不会改变的情况… Edge-weighted models是将动态网络转化为edge-weighted network 然后使用静态的GNN在上面。例如TDGNN Discrete dynamic graph neural networks给出一组离散的图通过GNN形成z代表在时间t中的i节点通过函数f，生成当前的h，f可以是RNN和自注意力机制也可以这样表示： Stacked Dynamic Graph Neural Networks","link":"/2021/09/14/%E5%8A%A8%E6%80%81%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%BC%E8%BF%B0%E7%AC%94%E8%AE%B0-%E4%BA%8C-%E5%8A%A8%E6%80%81%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"title":"通过右移2的幂次实现被除数为负数的除法","text":"在阅读CSAPP的时候，看了练习题3.20的答案，说到被除数为负数的时候需要加偏置。 WHY?总结了一下CSAPP中的解释（书P73）计算机实现除法的时候，需要的时钟周期很长，所以当除数为2的幂次的时候，就可以使用算术右移来实现除法。 被除数非负的情况整数的除法总是舍入到零的，比如2.3舍入到2， -1.5舍入到-1。当被除数为正数的时候，算术右移来执行除法，结果自然是向零舍入的。（因为直接把后面右移的数字舍弃了） 被除数负数的情况但是当被除数是负数的时候，会出现向下舍入，比如-1.5会被舍入为-2。为了解决这个问题，我们需要先对被除数加上一个偏置数。当除数为2的k次幂的时候，偏置量为2^k - 1这样就可以解决被除数为负数的舍入问题。以下为CSAPP中的推导 个人简单理解假设除数为2的k次幂，代表我们要右移k位，当这k位都为0的时候，代表不需要舍入，能够整除，但是当k位有任何一位为1的时候，代表不能整除，需要做舍入处理。如果不加偏置，就代表我们要直接舍弃最低的k位，对不需要右移的最低位毫无影响，那么对于负数来说。如果加上偏置，那么需要舍入的时候，对于第k+1位，会有一个来自低位的进位，这个时候数值自然就会向0舍入。","link":"/2021/07/19/%E9%80%9A%E8%BF%87%E5%8F%B3%E7%A7%BB2%E7%9A%84%E5%B9%82%E6%AC%A1%E5%AE%9E%E7%8E%B0%E9%99%A4%E6%B3%95/"},{"title":"计算机网络笔记3.7","text":"Classic TCP Congestion Control这种方法是当网络拥塞时，TCP减少发送的速率。当网络不拥塞时，TCP增加发送速率。这样就带来三个问题 当遇到网络阻塞时，TCP如何限制发送的速率？ TCP如何感知拥塞？ 当发送方感知到了端对端的拥塞时，应当采用什么算法改变速率？ 前提概念$LastByteRead$ 应用进程从缓存中独处的数据流的最后一个字节的编号$LastByteRcvd$ 从网络中到达的并且已放入主机接受缓存中的数据流最后一个字节的编号$rwnd$为接受窗口，是表明当前缓存中还有多少剩余的空间$$ rwnd = RcvBuffer -[LastByteRcvd - LastByteRead] $$ $ cwnd $ 为拥塞窗口，它对发送方向网络中发送流量的速率进行了限制 TCP发送方限制发送连接流量$ cwnd $ 会对一个TCP发送方能向防落中发送流量的速率进行限制。在一个发送方中未被确认的数据流不会超过$cwnd$和$rwnd$中的最小值。$$ LastByteSent - LastByteAcked \\le min{cwd, rwnd}$$如果rwnd足够大的时候，发送方能发送的未被确认的数据量取决于$cwdn$，所以通过调节$cwdn$的值，就可以调节发送方向它连接发送数据的速率。","link":"/2021/09/15/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%AC%94%E8%AE%B03-7/"},{"title":"记录BUG_孤儿进程","text":"今天写多进程的时候遇到了这样一个bug，我通过fork创建子进程，然后再让子进程fork创建子进程…直到一个终止条件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990// #include &quot;kernel/types.h&quot;// #include &quot;kernel/stat.h&quot;// #include &quot;user/user.h&quot;#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;stdlib.h&gt;#include &lt;sys/wait.h&gt;#define READMODE 0#define WRITEMODE 1int main(){ // int primers_array[40]; // for(int i = 2; i &lt;= 35; i++) // primers_array[i] = i; int p[2]; pipe(p); int read_pipe = p[READMODE]; int write_pipe = p[WRITEMODE]; close(0); int pppid = 0; if( (pppid = fork()) == 0) { while(1) { printf(&quot;pid: %d\\n&quot;, getpid()); pipe(p); close(write_pipe); int base_num; if(read(read_pipe, &amp;base_num, sizeof(int)) == 0) { printf(&quot;pid %d 我死了\\n&quot;,getpid()); close(p[READMODE]); close(p[WRITEMODE]); close(read_pipe); exit(0); } printf(&quot;prime %d\\n&quot;, base_num); if(base_num == 31) { close(p[READMODE]); close(p[WRITEMODE]); close(read_pipe); exit(0); } int pid = fork(); if(pid &gt; 0) { close(p[READMODE]); int x; while(read(read_pipe, &amp;x, sizeof(int))) { if(x % base_num != 0) { write(p[WRITEMODE], &amp;x, sizeof(int)); } } close(read_pipe); int status = 0; printf(&quot;pid %d done\\n&quot;, getpid()); exit(0); } else { read_pipe = p[READMODE]; write_pipe = p[WRITEMODE]; } } } else { close(p[READMODE]); for(int i = 2; i &lt;= 35; i++) { write(p[WRITEMODE], &amp;i, sizeof(int)); } close(p[WRITEMODE]); int wpid; int status = 0; printf(&quot;结束了&quot;); exit(0); } exit(0);} 发现最后进程都退出之后，命令行不动，除非输入一些东西或者放大缩小终端界面。。。我目测原因是因为这样链式fork，并且每个父进程都在子进程前面exit，就会导致子进程成为孤儿进程，孤儿进程由init操作系统管理，也就是说，孤儿进程的父进程会变成system，不再是bash，原因应该是最后一个进程exit后，bash没有关于这个进程的信息，所以不会变化…但是具体原因不是很清楚，需要学完操作系统后再看。而且当我使用wait之后，所有父进程都卡住了… An orphan process is a computer process whose parent process has finished or terminated, though it remains running itself. In a Unix-like operating system any orphaned process will be immediately adopted by the special init system process: the kernel sets the parent to init. This operation is called re-parenting and occurs automatically. Even though technically the process has the “init” process as its parent, it is still called an orphan process since the process that originally created it no longer exists. In other systems orphaned processes are immediately terminated by the kernel. In modern Linux systems, an orphan process may be reparented to a “subreaper” process instead of init.","link":"/2021/09/19/%E8%AE%B0%E5%BD%95BUG-%E5%AD%A4%E5%84%BF%E8%BF%9B%E7%A8%8B/"}],"tags":[{"name":"bug","slug":"bug","link":"/tags/bug/"}],"categories":[]}
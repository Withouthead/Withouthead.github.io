{"pages":[{"title":"关于","text":"关于我一名普通的学生。","link":"/about/index.html"}],"posts":[{"title":"通过右移2的幂次实现被除数为负数的除法","text":"在阅读CSAPP的时候，看了练习题3.20的答案，说到被除数为负数的时候需要加偏置。 WHY?总结了一下CSAPP中的解释（书P73）计算机实现除法的时候，需要的时钟周期很长，所以当除数为2的幂次的时候，就可以使用算术右移来实现除法。 被除数非负的情况整数的除法总是舍入到零的，比如2.3舍入到2， -1.5舍入到-1。当被除数为正数的时候，算术右移来执行除法，结果自然是向零舍入的。（因为直接把后面右移的数字舍弃了） 被除数负数的情况但是当被除数是负数的时候，会出现向下舍入，比如-1.5会被舍入为-2。为了解决这个问题，我们需要先对被除数加上一个偏置数。当除数为2的k次幂的时候，偏置量为2^k - 1这样就可以解决被除数为负数的舍入问题。以下为CSAPP中的推导 个人简单理解假设除数为2的k次幂，代表我们要右移k位，当这k位都为0的时候，代表不需要舍入，能够整除，但是当k位有任何一位为1的时候，代表不能整除，需要做舍入处理。如果不加偏置，就代表我们要直接舍弃最低的k位，对不需要右移的最低位毫无影响，那么对于负数来说。如果加上偏置，那么需要舍入的时候，对于第k+1位，会有一个来自低位的进位，这个时候数值自然就会向0舍入。","link":"/2021/07/19/%E9%80%9A%E8%BF%87%E5%8F%B3%E7%A7%BB2%E7%9A%84%E5%B9%82%E6%AC%A1%E5%AE%9E%E7%8E%B0%E9%99%A4%E6%B3%95/"},{"title":"《STL源码剖析》第二章问题杂烩","text":"近期开始阅读侯捷《STL源码剖析》，第二章有许许多多我遇到的问题，在此汇总一下 Placement new书中的使用 语法new的语法中有placement_params可选参数，当传递给new这个参数时，就是placement_new 作用placement_new就是直接将一块内存空间指定给new，new直接在这些内存上执行构造 样例1234567891011121314//样例1void* operator new(std::size_t, void*)//样例2// within any block scope...{ alignas(T) unsigned char buf[sizeof(T)]; // Statically allocate the storage with automatic storage duration // which is large enough for any object of type `T`. T* tptr = new(buf) T; // Construct a `T` object, placing it directly into your // pre-allocated storage at memory address `buf`. tptr-&gt;~T(); // You must **manually** call the object's destructor // if its side effects is depended by the program.} // Leaving this block scope automatically deallocates `buf`. false_type和true_type书中的使用书中是用来方便__destory_aux的调用，个人理解为重载，可以由__destroy直接调用两个不同作用的函数，通过false_type和true_type这两个不同的变量类型。 未完待续….","link":"/2021/08/18/STL%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90%E7%AC%AC%E4%BA%8C%E7%AB%A0%E9%97%AE%E9%A2%98%E6%9D%82%E7%83%A9/"},{"title":"《STL源码剖析》中void (* set_malloc_handler(void (*f)()))()分析","text":"问题在阅读《STL源码剖析》的时候，遇到了这么一个函数 先不说函数的意思，函数的形式我就没看懂…好多好多括号… 函数指针函数指针就是指向函数的指针… 1234567891011121314151617181920212223242526272829/* 例一：函式指標直接呼叫 复制自WIKI*/# ifndef __cplusplus # include &lt;stdio.h&gt;# else # include &lt;cstdio&gt;# endifint max(int x, int y){ return x &gt; y ? x : y;}int main(void){ /* p 是函数指针*/ int (* p)(int, int) = &amp; max; // &amp;可以省略 int a, b, c, d; printf(&quot;please input 3 numbers:&quot;); scanf(&quot;%d %d %d&quot;, &amp; a, &amp; b, &amp; c); /* 與直接呼叫函式等價，d = max(max(a, b), c) */ d = p(p(a, b), c); printf(&quot;the maxumum number is: %d\\n&quot;, d); return 0;} 在《STL源码剖析》里的这个函数，实际上使用了函数指针作为参数 1void (*f)() 函数指针作为返回值1int (*test(int))(int, int) 阅读方法是由内向外读，首先test有形参列表，所以是一个函数，并且参数只需要一个int，然后test的前面有一个*，所以返回一个指针，(*test(int))作为一个指针，有形参列表(int, int)，所以这个指针指向函数，并且这个函数返回int类型的值。 分析1void (* set_malloc_handler(void (*f)()))() 首先，set_malloc_handler作为函数名，有一个形参，是void(*f)()，void(f)()是一个函数指针类型，指向返回值为void，参数为空的函数。这说明set_malloc_handler是函数，并且前面有，所以要返回指针，指针后面接着形参列表，为空()，说明是指向函数的指针，并且指向的函数返回类型为void。","link":"/2021/08/20/%E5%87%BD%E6%95%B0%E6%8C%87%E9%92%88/"},{"title":"动态图神经网络综述笔记(一)分类","text":"本文是动态网络综述：Foundations and modelling of dynamicnetworks using Dynamic Graph NeuralNetworks: A survey 的笔记 分类根据时间粒度分类 Static没有时间信息 Edge-weighted时间信息被当做标签存放在静态网络的边/点中，最直观的例子是静态网络中边最后一次活跃时间的标签。 Discretue以离散的时间间隔表示时间信息，可以用不同时间间隔的快照来表示。 Continuous没有时间间隔，这种表示方式承载着最多的信息，但同时也最复杂。 后两者主要用来建立动态网络。 表示方法 Discrete RepresentationT代表快照的序号 Continuous Representation The event-basedrepresentationui和vi是一对连接的点，ti是时间戳，代表连接开始的时间，△i是事件(连接)持续的时间 The contact sequence representation是上一种的简化，在这种连接中，连接是instantaneous(瞬时)的，所以没有连接的持续时间。 The graph stream representation其中 ui,vi是一对连接的点，ti是事件发生的时间，最后的符号如果为1，代表边的的加入，-1代表边的删除。 根据连接时间分类 Interaction Temporal Evolving Strictly evolving 从上到下连接时间为0-无穷 根据点的动态性区分 Static点的数量始终不变 Dynamic点可能消失或出现 Growin是一种特殊的Dynamic，点只能增长 动态网络CUBEtemporal可以在没有连接的情况下存在，但对于evolving等许多网络来说不可以，当连接不存在时，点也就不存在了。三种分类可以形成一个3D的分类图 动态网络模型","link":"/2021/09/13/%E5%8A%A8%E6%80%81%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%BC%E8%BF%B0%E7%AC%94%E8%AE%B0(%E4%B8%80)/"},{"title":"动态图神经网络综述笔记(二)动态图神经网络","text":"笔记第二部分 Dynamic graph neural networks动态图网络有深度的时间序列编码，能将邻居节点聚合起来。 定义如果一个深度表征学习将邻居节点信息聚集起来作为结构，那么这就是一个动态图神经网络在离散的情况下，DGNN是GNN和时间序列的组合。如果是连续的，情况就会有些多变，聚集节点不能再用传统的GNN的方法。 DGNN的种类 Pseudo-dynamic这个方法改变的是网络的拓扑结构，而不是时间。 Discrete编码网络使用了快照，并且每次快照编码一次。 Continuous遍历网络是以边和边的形式，所以它完全独立于任何的快照尺寸。Pserudo-dynamic models pseudo-dynamic contains dynamic processes, but thedynamic properties of the model are not fit to the dynamicdata. 按照我的理解，这种方式只适用于数据不会改变的情况… Edge-weighted models是将动态网络转化为edge-weighted network 然后使用静态的GNN在上面。例如TDGNN Discrete dynamic graph neural networks给出一组离散的图通过GNN形成z代表在时间t中的i节点通过函数f，生成当前的h，f可以是RNN和自注意力机制也可以这样表示： Stacked Dynamic Graph Neural Networks","link":"/2021/09/14/%E5%8A%A8%E6%80%81%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%BC%E8%BF%B0%E7%AC%94%E8%AE%B0-%E4%BA%8C-%E5%8A%A8%E6%80%81%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"title":"计算机网络笔记3.7","text":"Classic TCP Congestion Control这种方法是当网络拥塞时，TCP减少发送的速率。当网络不拥塞时，TCP增加发送速率。这样就带来三个问题 当遇到网络阻塞时，TCP如何限制发送的速率？ TCP如何感知拥塞？ 当发送方感知到了端对端的拥塞时，应当采用什么算法改变速率？ 前提概念$LastByteRead$ 应用进程从缓存中独处的数据流的最后一个字节的编号$LastByteRcvd$ 从网络中到达的并且已放入主机接受缓存中的数据流最后一个字节的编号$rwnd$为接受窗口，是表明当前缓存中还有多少剩余的空间$$ rwnd = RcvBuffer -[LastByteRcvd - LastByteRead] $$ $ cwnd $ 为拥塞窗口，它对发送方向网络中发送流量的速率进行了限制 TCP发送方限制发送连接流量$ cwnd $ 会对一个TCP发送方能向防落中发送流量的速率进行限制。在一个发送方中未被确认的数据流不会超过$cwnd$和$rwnd$中的最小值。$$ LastByteSent - LastByteAcked \\le min{cwd, rwnd}$$如果rwnd足够大的时候，发送方能发送的未被确认的数据量取决于$cwdn$，所以通过调节$cwdn$的值，就可以调节发送方向它连接发送数据的速率。","link":"/2021/09/15/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%AC%94%E8%AE%B03-7/"},{"title":"CSAPP第七章Linking","text":"Object FilesObject file 有三种形式 Relocatable object file Executable object file Shared object file Relocatable Object FilesCSAPP 书中选取的是ELF格式。ELF从16字节的序列作为起始，这16字节规定了字的大小和机器的字节顺序(大端或者小端)，ELH header剩下的内容帮助连接器分析解释这个object file。而各种sections的信息，如位置和大小，由section header table 给出，它包含了每个sections固定大小的入口。 sections的类型 .te xt编译后的机器码 .rodata只读数据，例如printf中的format strings 或者switch的jump tables .data初始化的全局和静态变量 .bss未初始化的全局和静态变量，并且这个section并不占据object file的实际磁盘空间，当程序运行时，这些变量会申请内存并初始化为0 .symtabsymbol table，包含了函数和全局变量的信息。 .rel.rext是.text section中需要重定位的位置的列表，例如外部函数或者外部的局部变量。和并不包含没有用到的指令。 .rel.data包含这个object module中定义或者引用的全局变量的重定位信息。 .debugdebugging symbol，只有用了-g选项编译才会产生 .line .strtab","link":"/2021/09/16/CSAPP%E7%AC%AC%E4%B8%83%E7%AB%A0Linking/"},{"title":"Streaming Graph Neural Networks笔记","text":"Streaming Graph Neural Networks笔记 该论文提出了DGNN模型 interact unit$$ e(t)=act(W_1 \\cdot u_{v_s}(t-)+W_2 \\cdot u_{v_g}(t-)+b_e)$$ 其中$u_{v_s}和$u_{v_g}是点在时间t之前的特征，$W_1$和$W2$以及$b_e$是神经网络的参数,$act(\\cdot)$是激活函数。$e(t)$包含了${v_s, v_g, t}$的交互的信息。 update unitupdata unit将经过intercact unit的节点信息更新。当许多interactions作用于同一节点时，update unit将遗忘以前的interactions。这操作和LSTM很像，所以论文魔改了LSTM其中target node和source node是用的两个不同的update unit，结构相同，参数不同。update unit和LSTM不同的地方在蓝色虚线中，相当于改变了LSTM中$C$的输入。公式：$$C^I_v(t-1) = tanh(W_d \\cdot C_v(t-1)+b_d)$$ $$\\hat{C}^I_v(t-1)=C^I_v(t-1)*g(\\Delta t)$$ $$C^T_v(t-1)=C_v(t-1)-C^I_v(t-1)$$ $$C^*_v(t-1)=C^T_v(t-1)+\\hat{C}^I_v(t-1)$$其中$C^I_v$由神经网络生成，代表短期记忆，短期记忆会随着时间遗忘，所以短期记忆会乘上$g(\\Delta t)$ 得出$\\hat{C}^I_v$，来表示遗忘后的短期记忆，$g(\\Delta t)$是遗忘函数，$\\Delta t$越长，其值越小。$C^T_v$是长期记忆，值不随时间而改变。$C^*_v(t-1)$是调整之后的$C$，作为标准LSTM的输入。后面就是LSTM的内容了。","link":"/2021/09/18/Streaming-Graph-Neural-Networks%E7%AC%94%E8%AE%B0/"}],"tags":[],"categories":[]}